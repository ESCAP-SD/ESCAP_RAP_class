{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Web scraper walkthrough: review from Session 11\"\n",
    "author: Frances Krsinich\n",
    "date: August 21, 2024\n",
    "order: 7\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-expand: 2\n",
    "    code-fold: false\n",
    "    other-links:\n",
    "        - text: \"NHS guide on notebooks versus using an IDE\"\n",
    "          href: https://nhsdigital.github.io/rap-community-of-practice/implementing_RAP/notebooks_versus_ide_development/\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This page renders the jupyter notebook that Frances put together on August 21 to scrape ([Farmers.co.nz](https://www.farmers.co.nz/)).\n",
    "\n",
    "* Aim: to scrape all product names and prices for women's fashion from the Farmers.co.nz site\n",
    "\n",
    "As usual, one first need some packages to be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Initial scrape: Only one page\n",
    "Start with just one page - the first page for women's tops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, check that we can\n",
    "\n",
    "A common first step is to confirm whether we can scrape at all - so we should check out the [robots.txt](https://www.farmers.co.nz/robots.txt). \n",
    "\n",
    "```\n",
    "User-agent:*\n",
    "Disallow: /cms/post-surgery-bra-fitting\n",
    "Disallow: /cms/mothers-day-v2\n",
    "Disallow: /women/new-collection/sally-ann-mullin-s-top-picks\n",
    "Disallow: /cms/comp\n",
    "Disallow: /cms/testcataloguetest\n",
    "Disallow: /cms/page_SelectorTest_20190823084433\n",
    "Disallow: /cms/instore-bed-selector\n",
    "Disallow: /cms/stevens-privacy-policy\n",
    "Disallow: /cms/sremraf\n",
    "Disallow: /cms/store-update\n",
    "Disallow: /cms/sleepyhead-early-bird-voucher\n",
    "Disallow: *SearchTerm=*\n",
    "Disallow: /filter/*\n",
    "Disallow: */INTERSHOP/web*\n",
    "Disallow: */ManufacturerName-*\n",
    "Disallow: */ProductSalePriceGross-*\n",
    "Disallow: */Size-*\n",
    "Disallow: */FilterClearance-*\n",
    "Disallow: */ColourFamilyDisplayName-*\n",
    "Disallow: */function-*\n",
    "Disallow: */gender-*\n",
    "Disallow: */age-*\n",
    "Disallow: */SpecialOffer-*\n",
    "Disallow: */StockStatus-*\n",
    "Disallow: */styleshape-*\n",
    "Disallow: */fabric-*\n",
    "Disallow: */fillingtype-*\n",
    "Disallow: */GroupSize*\n",
    "Disallow: *ContextCategoryUUID*\n",
    "\n",
    "Sitemap: https://www.farmers.co.nz/sitemap-sitemap\n",
    "\n",
    "User-agent: bingbot\n",
    "Crawl-delay: 1\n",
    "```\n",
    "\n",
    "As the site we want to scrape is https://www.farmers.co.nz/women/fashion/tops, this is not disallowed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Website URL\n",
    "It is good to have a look at the website beforehand and anvigate a bit to see if the strucure looks easy to naviage, fand formating consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the website\n",
    "url = \"https://www.farmers.co.nz/women/fashion/tops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the website\n",
    "Then the URL of the website has to be tested. We send a request to the web server hosting the URL, asking for the content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the response, we can start scrapping (or not). The responses can be: \n",
    "* 200 OK, there are some elements (data in return of our request)\n",
    "* 404: Not Found, nothing is returned, there may be an error in the URL\n",
    "* other responses such as  500: Internal Server Error, or  403 forbidden to acces...\n",
    "\n",
    "Here the sesponse is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A code response = 200, means that the URL is correctly responding our request. We can then move on to scrap ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BeautifulSoup to start scraping\n",
    "The whole code will be embedded into an if then else structure:\n",
    "\n",
    "**if** one can scrap\n",
    "   ***(then)*** scrap website\n",
    "      (many actions there)\n",
    "      at the end print a message with the location of the file with data\n",
    "**else** do nothing\n",
    "      print a message informaing that nothing was retuned \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to farmers_women_tops_p1.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the product listings\n",
    "    products = soup.find_all(\"div\", class_ = \"product-tile\")\n",
    "    \n",
    "    # Create lists to store the data\n",
    "    product_names = []\n",
    "    product_prices = []\n",
    "    \n",
    "    # Loop through the product listings and extract the data\n",
    "    for product in products:\n",
    "        name_tag = product.find(\"span\", class_ = \"product-title-span\") # found by inspecting html\n",
    "        price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n",
    "        \n",
    "        if name_tag:\n",
    "            name = name_tag.text.strip()  \n",
    "        else:\n",
    "            name = \"N/A\"\n",
    "        \n",
    "        if price_tag:\n",
    "            price = price_tag.text.strip()\n",
    "        else:\n",
    "            price = \"N/A\"\n",
    "        \n",
    "        product_names.append(name)\n",
    "        product_prices.append(price)\n",
    "    \n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        \"Product Name\": product_names,\n",
    "        \"Product Price\": product_prices\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"farmers_women_tops_p1.csv\", index=False)\n",
    "    \n",
    "    print(\"Data has been written to farmers_women_tops_p1.csv\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oliver Black Animal Print V-Neck Short Sleeve ...</td>\n",
       "      <td>$69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ella J Ditsy Tie Top, Aqua</td>\n",
       "      <td>$69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whistle Rib V-Neck Flutter Sleeve Tee, Baby Blue</td>\n",
       "      <td>$59.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whistle V-Neck Top, Black</td>\n",
       "      <td>$69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ella J Tropical Tie Top, Blue</td>\n",
       "      <td>$69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Product Price\n",
       "0  Oliver Black Animal Print V-Neck Short Sleeve ...        $69.99\n",
       "1                         Ella J Ditsy Tie Top, Aqua        $69.99\n",
       "2   Whistle Rib V-Neck Flutter Sleeve Tee, Baby Blue        $59.99\n",
       "3                          Whistle V-Neck Top, Black        $69.99\n",
       "4                      Ella J Tropical Tie Top, Blue        $69.99"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That got 24 prices (one page) - next step is to extend across all pages for women's tops\n",
    "\n",
    "Call this new file farmers_women_tops.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the web scraped data frame\n",
    "By looking at the resulting data frame we observe that the variable ProdcutPrice has some text in it. \n",
    "We can use regular expressions to create a \"Price\" variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the price using regex\n",
    "df['Price'] = df['Product Price'].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some descriptive statistics\n",
    "We may be interested in a first overview of the data collected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         min   mean    50%    max  count\n",
      "Price  49.99  71.24  69.99  99.99   24.0\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics for the Price variable\n",
    "price_stats = df['Price'].describe()\n",
    "\n",
    "## Select the statistics we are interested in and display horizontally\n",
    "selected_stats = price_stats[[ 'min','mean', '50%' , 'max', 'count']].to_frame().T\n",
    "\n",
    "print(selected_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical distribution of the prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part: looping over pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to farmers_women_tops.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://www.farmers.co.nz/women/fashion/tops\"\n",
    "\n",
    "# Lists to store the product data\n",
    "product_names = []\n",
    "product_prices = []\n",
    "\n",
    "# Loop through each page (0 to 14 in this specific case) \n",
    "for page_num in range(0, 15): # note, 2nd number of range not included, so needs to be 14+1\n",
    "    # Modify the URL to include the page number\n",
    "    url = f\"{base_url}/Page-{page_num}-SortingAttribute-SortBy-asc\" # specific to the Farmers.co.nz site\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Find the product listings\n",
    "        products = soup.find_all(\"div\", class_=\"product-tile\") # found by inspecting html\n",
    "        \n",
    "        # Loop through the product listings and extract the data\n",
    "        for product in products:\n",
    "            name_tag = product.find(\"span\", class_=\"product-title-span\") # found by inspecting html\n",
    "            price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n",
    "            \n",
    "            # Extract the product name\n",
    "            if name_tag:\n",
    "                name = name_tag.text.strip()  \n",
    "            else:\n",
    "                name = \"N/A\"\n",
    "            \n",
    "            # extract the product price\n",
    "            if price_tag:\n",
    "                price = price_tag.text.strip()\n",
    "            else:\n",
    "                price = \"N/A\"\n",
    "            \n",
    "            # Append the data to the lists\n",
    "            product_names.append(name)\n",
    "            product_prices.append(price)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page {page_num}\")\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    \"Product Name\": product_names,\n",
    "    \"Product Price\": product_prices\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"farmers_women_tops.csv\", index=False)\n",
    "\n",
    "print(\"Data has been written to farmers_women_tops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 360 entries, 0 to 359\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Product Name   360 non-null    object\n",
      " 1   Product Price  360 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully scraped all 348 women's tops.\n",
    "\n",
    "Next need to work out how to loop across categories (i.e. 'new arrivals', 'dresses', 'tops'...). Work out how to get the URLs for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_urls list has been created\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the website\n",
    "url = \"https://www.farmers.co.nz/women/fashion\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the categories\n",
    "    categories = soup.find_all(\"a\", class_ = \"category-list-image\")\n",
    "    \n",
    "    # Create list to store the data\n",
    "    category_urls = []\n",
    "\n",
    "    # Loop through the categories and extract the names\n",
    "    for category in categories:\n",
    "        url = category.get(\"href\", \"N/A\")\n",
    "        category_urls.append(url)\n",
    "          \n",
    "    print(\"category_urls list has been created\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.farmers.co.nz/women/fashion/new-arrivals',\n",
       " 'http://www.farmers.co.nz/women/fashion/dresses',\n",
       " 'http://www.farmers.co.nz/women/fashion/tops',\n",
       " 'http://www.farmers.co.nz/women/fashion/skirts',\n",
       " 'http://www.farmers.co.nz/women/fashion/jeans',\n",
       " 'http://www.farmers.co.nz/women/fashion/pants-leggings',\n",
       " 'http://www.farmers.co.nz/women/fashion/activewear',\n",
       " 'http://www.farmers.co.nz/women/fashion/shorts',\n",
       " 'http://www.farmers.co.nz/women/fashion/swimwear',\n",
       " 'http://www.farmers.co.nz/women/fashion/sweatshirts-hoodies',\n",
       " 'http://www.farmers.co.nz/women/fashion/knitwear',\n",
       " 'http://www.farmers.co.nz/women/fashion/coats-jackets']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work out how to get the number of pages as a variable so it doesn't have to be hard-coded as above for tops (which had pages 0-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved number of pages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://www.farmers.co.nz/women/fashion/dresses\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(base_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the number of pages to be iterated through\n",
    "    pagenum_tag = soup.find_all(\"span\", class_ = \"pagination-hide\")\n",
    "\n",
    "    lastpage = pagenum_tag[-1].text.strip()  \n",
    "\n",
    "    lastpage_num = int(lastpage[2:])\n",
    "    print(\"retrieved number of pages\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try and combine the outer loop above with the code that scrapes all the pages (after determining number of pages) for each of the categories.\n",
    "And remember to add in a variable which has the date and time of the scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_urls list has been created\n",
      "Number of pages is 7\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/new-arrivals\n",
      "Number of pages is 5\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/dresses\n",
      "Number of pages is 19\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/tops\n",
      "Number of pages is 3\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/skirts\n",
      "Number of pages is 3\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/jeans\n",
      "Number of pages is 6\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/pants-leggings\n",
      "Number of pages is 9\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/activewear\n",
      "Number of pages is 0\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/shorts\n",
      "Number of pages is 2\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/swimwear\n",
      "Number of pages is 0\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/sweatshirts-hoodies\n",
      "Number of pages is 3\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/knitwear\n",
      "Number of pages is 3\n",
      "Data has been written to farmers_womens_fashion.csv for page http://www.farmers.co.nz/women/fashion/coats-jackets\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "#import pandas as pd\n",
    "from datetime import datetime # need this new module\n",
    "\n",
    "# Define the URL of the website\n",
    "url = \"https://www.farmers.co.nz/women/fashion\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the categories\n",
    "    categories = soup.find_all(\"a\", class_ = \"category-list-image\")\n",
    "    \n",
    "    # Create list to store the data\n",
    "    category_urls = []\n",
    "\n",
    "    # Loop through the categories and extract the names\n",
    "    for category in categories:\n",
    "        url = category.get(\"href\", \"N/A\")\n",
    "        category_urls.append(url)\n",
    "          \n",
    "    print(\"category_urls list has been created\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")   \n",
    "\n",
    "# now run a loop across the list of categories (dresses, tops etc ) \n",
    "\n",
    "# Lists to store the product data\n",
    "product_names = []\n",
    "product_prices = []\n",
    "product_urls = [] # get the full url for now can transform at later stage\n",
    "scrape_times = [] \n",
    "    \n",
    "for category_url in category_urls:\n",
    "    \n",
    "    # Base URL for the product category\n",
    "    base_url = category_url\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "        # Find the number of pages to be iterated through\n",
    "        pagenum_tag = soup.find_all(\"span\", class_ = \"pagination-hide\")\n",
    "        if pagenum_tag == [] :\n",
    "            lastpage_num = 0\n",
    "        \n",
    "        else : \n",
    "\n",
    "            lastpage = pagenum_tag[-1].text.strip()  \n",
    "            lastpage_num = int(lastpage[2:])\n",
    "\n",
    "        print(f\"Number of pages is {lastpage_num}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")    \n",
    "\n",
    "    for page_num in range(0, lastpage_num):\n",
    "        # Modify the URL to include the page number\n",
    "        url = f\"{base_url}/Page-{page_num}-SortingAttribute-SortBy-asc\"\n",
    "    \n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "            # Find the product listings\n",
    "            products = soup.find_all(\"div\", class_=\"product-tile\")\n",
    "        \n",
    "            # Loop through the product listings and extract the data\n",
    "            for product in products:\n",
    "                name_tag = product.find(\"span\", class_=\"product-title-span\")\n",
    "                price_tag = product.find(\"div\", class_=\"current-price\")\n",
    "            \n",
    "                # Extract and clean the product name\n",
    "                name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "            \n",
    "                # Extract and clean the product price\n",
    "                price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "                # get time of scrape\n",
    "                scrape_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "                # Append the data to the lists\n",
    "                product_names.append(name)\n",
    "                product_prices.append(price)\n",
    "                product_urls.append(base_url)\n",
    "                scrape_times.append(scrape_time)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page {page_num}\")\n",
    "\n",
    "        # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        \"Product Name\": product_names,\n",
    "        \"Product Price\": product_prices,\n",
    "        \"Product Url\" : product_urls, # note this is actually the category URL (eg 'tops') so will help with categorisation\n",
    "        \"Scrape Time\" : scrape_times\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(\"farmers_womens_fashion.csv\", index=False)\n",
    "\n",
    "    print(f\"Data has been written to farmers_womens_fashion.csv for page {category_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1293 entries, 0 to 1292\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Product Name   1293 non-null   object\n",
      " 1   Product Price  1293 non-null   object\n",
      " 2   Product Url    1293 non-null   object\n",
      " 3   Scrape Time    1293 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 40.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! that means we've scraped 1293 products and have a clean data frame that has our 4 columns!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
