{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Web scraping example\"\n",
    "author: Serge Goussev\n",
    "date: Sept 18, 2024\n",
    "order: 7\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-expand: 2\n",
    "    code-fold: false\n",
    "    other-links:\n",
    "        - text: \"A blog listing some popular sites with which you can practice web scraping\"\n",
    "          href: https://proxyway.com/guides/best-websites-to-practice-your-web-scraping-skills\n",
    "        - text: \"NHS guide on notebooks versus using an IDE - as we are using a notebook to explore the site but the notebook is less applicable for production\"\n",
    "          href: https://nhsdigital.github.io/rap-community-of-practice/implementing_RAP/notebooks_versus_ide_development/\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Similar to other examples that we had in the course - this notebook will demo how to scrape [Books to scrape](https://books.toscrape.com/index.html) - a fictional demo site that shows book prices and is designed for teaching web scraping! As it looks like a retailer website!\n",
    "\n",
    "## Can we scrape at all?\n",
    "\n",
    "Before staring out - we normally check whether we can scrape the site. To validate whether this site allows scraping - let's check this as well. This way we also demonstrate common steps:\n",
    "* Firstly we can check [https://books.toscrape.com/robots.txt](https://books.toscrape.com/robots.txt) for the site - which does not exist! \n",
    "* Secondly we check the Terms and Conditions on the site - however this site does not have anything. Searching the internet tells us that [this is a popular scraping sandbox for beginners](https://proxyway.com/guides/best-websites-to-practice-your-web-scraping-skills)!\n",
    "\n",
    "Thus we can proceed!\n",
    "\n",
    "----------------------\n",
    "\n",
    "# Explore the site\n",
    "\n",
    "First off - lets import the necessary things into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's set up the user agent that will be set up with the scraper request. We can build off [what we saw in Session 10 for inspiration](https://xtophe.notion.site/Web-Scraping-for-CPI-Training-c36b27c2ea6b4f02ad760069afaf6fb5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = {\n",
    "    'User-Agent':'ESCAP Webscraping RAP demo scraper 1.0',\n",
    "    'email': 'goussev.serge@gmail.com',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'}\n",
    "\n",
    "s = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation and how to get the data\n",
    "\n",
    "Firstly, using the developer mode in the browser, we see that there doesn't seem to be an API behind this site as only the `html` is delivered. Thus we will need to use `BeautifulSoup` and scrape the full `html`.\n",
    "\n",
    "To navigate the `html` aspect of the site, it looks like there are two ways we can do it:\n",
    "\n",
    "### Single-shot approach and pagination\n",
    "It looks like off the bat, the site lists all books together with a `next` page at the bottom right.\n",
    "\n",
    "When we start on page 1, within the html we see a `pager` class and `next` class within it:\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_page_1.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_pagination_html.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "<!-- \n",
    "![](../docs/images/books_to_scrape_approach_page_1.png)\n",
    "![](../docs/images/books_to_scrape_approach_pagination_html.png) -->\n",
    "\n",
    "Once we get to the end, we see only a `previous` and the `next` class is no longer there\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_pagination_last_page.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_pagination_html_last_page.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "\n",
    "<!-- ![](../docs/images/books_to_scrape_pagination_last_page.png)\n",
    "![](../docs/images/books_to_scrape_approach_pagination_html_last_page.png) -->\n",
    "\n",
    "The URL also starts behaves in a stable way, whether for all books or for a specific categories:\n",
    "* https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
    "* https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html\n",
    "\n",
    "This means that we can iterate through the pages quite easily. Since all the books are also available from the main page - we could just iterate through all 50 pages and get all 1000 books on the site.\n",
    "\n",
    "### Category-by-category approach\n",
    "We can also navigate the category section on the left and then iterate through all the pages for the category:\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_2.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"350\">\n",
    "\n",
    "<!-- ![](../docs/images/books_to_scrape_approach_2.png) -->\n",
    "\n",
    "The html class for this is `side_categories` and we should easily be able to get into the unordered list and iterate through all of the categories:\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_2_html.png\" alt=\"HTML listing the categories\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "<!-- ![](../docs/images/books_to_scrape_approach_2_html.png) -->\n",
    "\n",
    "### Which way should we go?\n",
    "As many retailer websites around the world NSOs will need to scrape will likely not get a single home page that lists all products - the more realistic scenario is to iterate first through each category, and then through each page on that category.\n",
    "\n",
    "\n",
    "## Scraping info on each individual book page\n",
    "\n",
    "On each individual page, there are likely several categories that are of interest to us for consumer price statistics:\n",
    "* Product name\n",
    "* Product category\n",
    "* Product description \n",
    "* UPC\n",
    "* Final (post-tax) price\n",
    "* Whether the product is available or not\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_individidual_page_info_of_interest.png\" alt=\"HTML listing the categories\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"800\">\n",
    "\n",
    "Below we will go through how to get all this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring how to scrape the site\n",
    "\n",
    "## Scraping categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL\n",
    "shop_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Use the with clause we learned about (could also be done directly) to collect and parse the site\n",
    "with s.get(shop_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the navigation we found previously (i.e. the `side_categories` div class), lets find all the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Books': 'catalogue/category/books_1/index.html',\n",
       " 'Travel': 'catalogue/category/books/travel_2/index.html',\n",
       " 'Mystery': 'catalogue/category/books/mystery_3/index.html',\n",
       " 'Historical Fiction': 'catalogue/category/books/historical-fiction_4/index.html',\n",
       " 'Sequential Art': 'catalogue/category/books/sequential-art_5/index.html',\n",
       " 'Classics': 'catalogue/category/books/classics_6/index.html',\n",
       " 'Philosophy': 'catalogue/category/books/philosophy_7/index.html',\n",
       " 'Romance': 'catalogue/category/books/romance_8/index.html',\n",
       " 'Womens Fiction': 'catalogue/category/books/womens-fiction_9/index.html',\n",
       " 'Fiction': 'catalogue/category/books/fiction_10/index.html',\n",
       " 'Childrens': 'catalogue/category/books/childrens_11/index.html',\n",
       " 'Religion': 'catalogue/category/books/religion_12/index.html',\n",
       " 'Nonfiction': 'catalogue/category/books/nonfiction_13/index.html',\n",
       " 'Music': 'catalogue/category/books/music_14/index.html',\n",
       " 'Default': 'catalogue/category/books/default_15/index.html',\n",
       " 'Science Fiction': 'catalogue/category/books/science-fiction_16/index.html',\n",
       " 'Sports and Games': 'catalogue/category/books/sports-and-games_17/index.html',\n",
       " 'Add a comment': 'catalogue/category/books/add-a-comment_18/index.html',\n",
       " 'Fantasy': 'catalogue/category/books/fantasy_19/index.html',\n",
       " 'New Adult': 'catalogue/category/books/new-adult_20/index.html',\n",
       " 'Young Adult': 'catalogue/category/books/young-adult_21/index.html',\n",
       " 'Science': 'catalogue/category/books/science_22/index.html',\n",
       " 'Poetry': 'catalogue/category/books/poetry_23/index.html',\n",
       " 'Paranormal': 'catalogue/category/books/paranormal_24/index.html',\n",
       " 'Art': 'catalogue/category/books/art_25/index.html',\n",
       " 'Psychology': 'catalogue/category/books/psychology_26/index.html',\n",
       " 'Autobiography': 'catalogue/category/books/autobiography_27/index.html',\n",
       " 'Parenting': 'catalogue/category/books/parenting_28/index.html',\n",
       " 'Adult Fiction': 'catalogue/category/books/adult-fiction_29/index.html',\n",
       " 'Humor': 'catalogue/category/books/humor_30/index.html',\n",
       " 'Horror': 'catalogue/category/books/horror_31/index.html',\n",
       " 'History': 'catalogue/category/books/history_32/index.html',\n",
       " 'Food and Drink': 'catalogue/category/books/food-and-drink_33/index.html',\n",
       " 'Christian Fiction': 'catalogue/category/books/christian-fiction_34/index.html',\n",
       " 'Business': 'catalogue/category/books/business_35/index.html',\n",
       " 'Biography': 'catalogue/category/books/biography_36/index.html',\n",
       " 'Thriller': 'catalogue/category/books/thriller_37/index.html',\n",
       " 'Contemporary': 'catalogue/category/books/contemporary_38/index.html',\n",
       " 'Spirituality': 'catalogue/category/books/spirituality_39/index.html',\n",
       " 'Academic': 'catalogue/category/books/academic_40/index.html',\n",
       " 'Self Help': 'catalogue/category/books/self-help_41/index.html',\n",
       " 'Historical': 'catalogue/category/books/historical_42/index.html',\n",
       " 'Christian': 'catalogue/category/books/christian_43/index.html',\n",
       " 'Suspense': 'catalogue/category/books/suspense_44/index.html',\n",
       " 'Short Stories': 'catalogue/category/books/short-stories_45/index.html',\n",
       " 'Novels': 'catalogue/category/books/novels_46/index.html',\n",
       " 'Health': 'catalogue/category/books/health_47/index.html',\n",
       " 'Politics': 'catalogue/category/books/politics_48/index.html',\n",
       " 'Cultural': 'catalogue/category/books/cultural_49/index.html',\n",
       " 'Erotica': 'catalogue/category/books/erotica_50/index.html',\n",
       " 'Crime': 'catalogue/category/books/crime_51/index.html'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus just on the section we want\n",
    "side_category_section = response.find(\"div\", class_ = \"side_categories\")\n",
    "\n",
    "# Isolate all the categories using the link tag as they will have a link\n",
    "categories = side_category_section.find_all('a')\n",
    "\n",
    "# Iterate through all the categories and save the link in a dictionary key-value pair assigned\n",
    "# to the name of the category itself\n",
    "dictionary_of_categories = {}\n",
    "for category in categories:\n",
    "    dictionary_of_categories[category.text.strip()] = category.get('href')\n",
    "\n",
    "# Now lets see what we were able to scrape\n",
    "dictionary_of_categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Appending each category to the site URL will give us a way how to navigate to the category!\n",
    "\n",
    "We can also probably remove the `index.html` from the end of each as it is detrimental to the perfomance of the site.\n",
    "\n",
    "## Navigate all pages and save the product (book) \n",
    "\n",
    "Lets say we want to focus on https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html. as there are 75 books to scrape within this category (others should look the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the product URL so that we can go there later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the main site URL + the category we want in the catalogue\n",
    "category_url = \"catalogue/category/books/sequential-art_5/\"\n",
    "with s.get(shop_url + category_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = response.find_all(\"article\", class_=\"product_pod\")\n",
    "# Extract the first link (which happens to be the picture), although we can get the second link too\n",
    "products[0].find('a').get('href')\n",
    "\n",
    "# as the full site per book is \n",
    "# https://books.toscrape.com/catalogue/scott-pilgrims-......html\n",
    "# we should thus strip ../../.. but keep the catalogue\n",
    "shop_url+'catalogue'+products[0].find('a').get('href')[8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! So we now know all the URLs per product\n",
    "\n",
    "### Finding the number of pages to go through\n",
    "\n",
    "Now we need to know how to iterate through pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'75 results - showing 1 to 20.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is no 'number of pages' to check, but we can estimate the number from \n",
    "# the amount of results at the top left of the page. For instance:\n",
    "response.find(\"form\", class_=\"form-horizontal\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting with spaces and then extracting the first item in the list gets us the number by category\n",
    "number_of_products = response.find(\"form\", class_=\"form-horizontal\").text.strip().split(\" \")[0]\n",
    "#knowing that each page displays 20 products and rounding up gives us the number of pages\n",
    "round(int(number_of_products)/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Did not need the below - ended up going another way\n",
    "\n",
    "# # Get the pager class\n",
    "# pager = response.find('ul', class_=\"pager\")\n",
    "# # Extract first the list from this pager class, then find the link and extract that\n",
    "# next_page_tag = pager.find('li', class_='next')\n",
    "# #as the next page tag may be missing, check if it was found\n",
    "# if next_page_tag != None:\n",
    "#     # if it was found, then extract it\n",
    "#     next_page_url = next_page_tag.find('a').get('href')\n",
    "# else:\n",
    "#     # else specify a None which we will look for when iterating through pages\n",
    "#     next_page_url = None\n",
    "# shop_url+category_url+next_page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting this all together to scrape the whole category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the main site URL + the category we want in the catagory of interest\n",
    "category_url = \"catalogue/category/books/sequential-art_5/\"\n",
    "with s.get(shop_url + category_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# create empty list to save URLs of each product/book to scrape into a list\n",
    "products_to_scrape = []\n",
    "\n",
    "# Find the number of pages to iterate through:\n",
    "number_of_products = response.find(\"form\", class_=\"form-horizontal\").text.strip().split(\" \")[0]\n",
    "# iterate through the list of pages\n",
    "for page in range(0,round(int(number_of_products)/20)):\n",
    "    # as we start off scraping the category site anyway, then we don't need to scrape it again\n",
    "    # however if we are now on page 2, we haven't yet scraped it so we should\n",
    "    if page > 0:\n",
    "        category_url = \"catalogue/category/books/sequential-art_5/page-{}.html\".format(page+1)\n",
    "        with s.get(shop_url + category_url, headers=heads) as res:\n",
    "            response = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # Find all the product pods on this page\n",
    "    products = response.find_all(\"article\", class_=\"product_pod\")\n",
    "    # Extract the first link (which happens to be the picture), although we can get the second link too\n",
    "    for product in products:\n",
    "        href = product.find('a').get('href')\n",
    "        products_to_scrape.append(shop_url+'catalogue'+product.find('a').get('href')[8:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',\n",
       " 'https://books.toscrape.com/catalogue/tsubasa-world-chronicle-2-tsubasa-world-chronicle-2_949/index.html',\n",
       " 'https://books.toscrape.com/catalogue/this-one-summer_947/index.html']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 3 products\n",
    "products_to_scrape[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check the number sraped\n",
    "len(products_to_scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As this page has 75 products/books - we scraped them all!\n",
    "\n",
    "## Scaping the individual product page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the main site URL + the category we want in the catagory of interest\n",
    "category_url = \"catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\"\n",
    "with s.get(shop_url + category_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get product (i.e. book) title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mesaerion: The Best Science Fiction Stories 1800-1849'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get product/book name\n",
    "response.title.text.split(\"|\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Product description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get product information\n",
    "\n",
    "As this is a table, there are 2 ways of getting this information, via `BeautifulSoup` and via `pandas`:\n",
    "\n",
    "**`BeautifulSoup` approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UPC': 'e30f54cea9b38190',\n",
       " 'Product Type': 'Books',\n",
       " 'Price (excl. tax)': 'Â£37.59',\n",
       " 'Price (incl. tax)': 'Â£37.59',\n",
       " 'Tax': 'Â£0.00',\n",
       " 'Availability': 'In stock (19 available)',\n",
       " 'Number of reviews': '0'}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the table into a dictionary\n",
    "product_info = {}\n",
    "\n",
    "# filter to the product page\n",
    "product_page = response.find(\"article\", class_=\"product_page\")\n",
    "# as this is the only table with rows, we can just get find all rows on the product page\n",
    "rows = product_page.find_all('tr')\n",
    "for row in rows:\n",
    "    # focus on each row's cells and then save the values\n",
    "    cells = row.find_all(['th', 'td'])\n",
    "    product_info[cells[0].text] = cells[1].text\n",
    "    \n",
    "# have a look at what we saved\n",
    "product_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`pandas` approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# all_tables = pd.read_html(shop_url + category_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
