{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Web scraping example\"\n",
    "date: Sept 18, 2024\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-expand: 2\n",
    "    code-fold: false\n",
    "    other-links:\n",
    "        - text: \"A blog listing some popular sites with which you can practice web scraping\"\n",
    "          href: https://proxyway.com/guides/best-websites-to-practice-your-web-scraping-skills\n",
    "        - text: \"NHS guide on notebooks versus using an IDE - as we are using a notebook to explore the site but the notebook is less applicable for production\"\n",
    "          href: https://nhsdigital.github.io/rap-community-of-practice/implementing_RAP/notebooks_versus_ide_development/\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Similar to other examples that we had in the course - this notebook will demo how to scrape [Books to scrape](https://books.toscrape.com/index.html) - a fictional demo site that shows book prices and is designed for teaching web scraping! As it looks like a retailer website!\n",
    "\n",
    "## Can we scrape at all?\n",
    "\n",
    "Before staring out - we normally check whether we can scrape the site. To validate whether this site allows scraping - let's check this as well. This way we also demonstrate common steps:\n",
    "* Firstly we can check [https://books.toscrape.com/robots.txt](https://books.toscrape.com/robots.txt) for the site - which does not exist! \n",
    "* Secondly we check the Terms and Conditions on the site - however this site does not have anything. Searching the internet tells us that [this is a popular scraping sandbox for beginners](https://proxyway.com/guides/best-websites-to-practice-your-web-scraping-skills)!\n",
    "\n",
    "Thus we can proceed!\n",
    "\n",
    "----------------------\n",
    "\n",
    "# Explore the site\n",
    "\n",
    "First off - lets import the necessary things into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's set up the user agent that will be set up with the scraper request. We can build off [what we saw in Session 10 for inspiration](https://xtophe.notion.site/Web-Scraping-for-CPI-Training-c36b27c2ea6b4f02ad760069afaf6fb5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = {\n",
    "    'User-Agent':'ESCAP Webscraping RAP demo scraper 1.0',\n",
    "    'email': 'example@gmail.com',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'}\n",
    "\n",
    "s = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation and how to get the data\n",
    "\n",
    "Firstly, using the developer mode in the browser, we see that there doesn't seem to be an API behind this site as only the `html` is delivered. Thus we will need to use `BeautifulSoup` and scrape the full `html`.\n",
    "\n",
    "To navigate the `html` aspect of the site, it looks like there are two ways we can do it:\n",
    "\n",
    "### Single-shot approach and pagination\n",
    "It looks like off the bat, the site lists all books together with a `next` page at the bottom right.\n",
    "\n",
    "When we start on page 1, within the html we see a `pager` class and `next` class within it:\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_page_1.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_pagination_html.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "<!-- \n",
    "![](../docs/images/books_to_scrape_approach_page_1.png)\n",
    "![](../docs/images/books_to_scrape_approach_pagination_html.png) -->\n",
    "\n",
    "Once we get to the end, we see only a `previous` and the `next` class is no longer there\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_pagination_last_page.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_pagination_html_last_page.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "\n",
    "<!-- ![](../docs/images/books_to_scrape_pagination_last_page.png)\n",
    "![](../docs/images/books_to_scrape_approach_pagination_html_last_page.png) -->\n",
    "\n",
    "The URL also starts behaves in a stable way, whether for all books or for a specific categories:\n",
    "* https://books.toscrape.com/catalogue/category/books_1/page-2.html\n",
    "* https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html\n",
    "\n",
    "This means that we can iterate through the pages quite easily. Since all the books are also available from the main page - we could just iterate through all 50 pages and get all 1000 books on the site.\n",
    "\n",
    "### Category-by-category approach\n",
    "We can also navigate the category section on the left and then iterate through all the pages for the category:\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_2.png\" alt=\"Alternative approach using navigation\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"350\">\n",
    "\n",
    "<!-- ![](../docs/images/books_to_scrape_approach_2.png) -->\n",
    "\n",
    "The html class for this is `side_categories` and we should easily be able to get into the unordered list and iterate through all of the categories:\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_approach_2_html.png\" alt=\"HTML listing the categories\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"500\">\n",
    "<!-- ![](../docs/images/books_to_scrape_approach_2_html.png) -->\n",
    "\n",
    "### Which way should we go?\n",
    "As many retailer websites around the world NSOs will need to scrape will likely not get a single home page that lists all products - the more realistic scenario is to iterate first through each category, and then through each page on that category.\n",
    "\n",
    "\n",
    "## Scraping info on each individual book page\n",
    "\n",
    "On each individual page, there are likely several categories that are of interest to us for consumer price statistics:\n",
    "* Product name\n",
    "* Product category\n",
    "* Product description \n",
    "* UPC\n",
    "* Final (post-tax) price\n",
    "* Whether the product is available or not\n",
    "\n",
    "<img src=\"../docs/images/books_to_scrape_individidual_page_info_of_interest.png\" alt=\"HTML listing the categories\" style=\"box-shadow: 5px 5px 5px gray;\" width=\"800\">\n",
    "\n",
    "Below we will go through how to get all this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring how to scrape the site\n",
    "\n",
    "## Scraping categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL\n",
    "shop_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Use the with clause we learned about (could also be done directly) to collect and parse the site\n",
    "with s.get(shop_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response) == BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the navigation we found previously (i.e. the `side_categories` div class), lets find all the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Books': 'catalogue/category/books_1/index.html',\n",
       " 'Travel': 'catalogue/category/books/travel_2/index.html',\n",
       " 'Mystery': 'catalogue/category/books/mystery_3/index.html',\n",
       " 'Historical Fiction': 'catalogue/category/books/historical-fiction_4/index.html',\n",
       " 'Sequential Art': 'catalogue/category/books/sequential-art_5/index.html',\n",
       " 'Classics': 'catalogue/category/books/classics_6/index.html',\n",
       " 'Philosophy': 'catalogue/category/books/philosophy_7/index.html',\n",
       " 'Romance': 'catalogue/category/books/romance_8/index.html',\n",
       " 'Womens Fiction': 'catalogue/category/books/womens-fiction_9/index.html',\n",
       " 'Fiction': 'catalogue/category/books/fiction_10/index.html',\n",
       " 'Childrens': 'catalogue/category/books/childrens_11/index.html',\n",
       " 'Religion': 'catalogue/category/books/religion_12/index.html',\n",
       " 'Nonfiction': 'catalogue/category/books/nonfiction_13/index.html',\n",
       " 'Music': 'catalogue/category/books/music_14/index.html',\n",
       " 'Default': 'catalogue/category/books/default_15/index.html',\n",
       " 'Science Fiction': 'catalogue/category/books/science-fiction_16/index.html',\n",
       " 'Sports and Games': 'catalogue/category/books/sports-and-games_17/index.html',\n",
       " 'Add a comment': 'catalogue/category/books/add-a-comment_18/index.html',\n",
       " 'Fantasy': 'catalogue/category/books/fantasy_19/index.html',\n",
       " 'New Adult': 'catalogue/category/books/new-adult_20/index.html',\n",
       " 'Young Adult': 'catalogue/category/books/young-adult_21/index.html',\n",
       " 'Science': 'catalogue/category/books/science_22/index.html',\n",
       " 'Poetry': 'catalogue/category/books/poetry_23/index.html',\n",
       " 'Paranormal': 'catalogue/category/books/paranormal_24/index.html',\n",
       " 'Art': 'catalogue/category/books/art_25/index.html',\n",
       " 'Psychology': 'catalogue/category/books/psychology_26/index.html',\n",
       " 'Autobiography': 'catalogue/category/books/autobiography_27/index.html',\n",
       " 'Parenting': 'catalogue/category/books/parenting_28/index.html',\n",
       " 'Adult Fiction': 'catalogue/category/books/adult-fiction_29/index.html',\n",
       " 'Humor': 'catalogue/category/books/humor_30/index.html',\n",
       " 'Horror': 'catalogue/category/books/horror_31/index.html',\n",
       " 'History': 'catalogue/category/books/history_32/index.html',\n",
       " 'Food and Drink': 'catalogue/category/books/food-and-drink_33/index.html',\n",
       " 'Christian Fiction': 'catalogue/category/books/christian-fiction_34/index.html',\n",
       " 'Business': 'catalogue/category/books/business_35/index.html',\n",
       " 'Biography': 'catalogue/category/books/biography_36/index.html',\n",
       " 'Thriller': 'catalogue/category/books/thriller_37/index.html',\n",
       " 'Contemporary': 'catalogue/category/books/contemporary_38/index.html',\n",
       " 'Spirituality': 'catalogue/category/books/spirituality_39/index.html',\n",
       " 'Academic': 'catalogue/category/books/academic_40/index.html',\n",
       " 'Self Help': 'catalogue/category/books/self-help_41/index.html',\n",
       " 'Historical': 'catalogue/category/books/historical_42/index.html',\n",
       " 'Christian': 'catalogue/category/books/christian_43/index.html',\n",
       " 'Suspense': 'catalogue/category/books/suspense_44/index.html',\n",
       " 'Short Stories': 'catalogue/category/books/short-stories_45/index.html',\n",
       " 'Novels': 'catalogue/category/books/novels_46/index.html',\n",
       " 'Health': 'catalogue/category/books/health_47/index.html',\n",
       " 'Politics': 'catalogue/category/books/politics_48/index.html',\n",
       " 'Cultural': 'catalogue/category/books/cultural_49/index.html',\n",
       " 'Erotica': 'catalogue/category/books/erotica_50/index.html',\n",
       " 'Crime': 'catalogue/category/books/crime_51/index.html'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus just on the section we want\n",
    "side_category_section = response.find(\"div\", class_ = \"side_categories\")\n",
    "\n",
    "# Isolate all the categories using the link tag as they will have a link\n",
    "categories = side_category_section.find_all('a')\n",
    "\n",
    "# Iterate through all the categories and save the link in a dictionary key-value pair assigned\n",
    "# to the name of the category itself\n",
    "dictionary_of_categories = {}\n",
    "for category in categories:\n",
    "    dictionary_of_categories[category.text.strip()] = category.get('href')\n",
    "\n",
    "# Now lets see what we were able to scrape\n",
    "dictionary_of_categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Appending each category to the site URL will give us a way how to navigate to the category!\n",
    "\n",
    "We can also probably remove the `index.html` from the end of each as it is detrimental to the perfomance of the site.\n",
    "\n",
    "## Navigate all pages and save the product (book) \n",
    "\n",
    "Lets say we want to focus on https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html. as there are 75 books to scrape within this category (others should look the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the product URL so that we can go there later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the main site URL + the category we want in the catalogue\n",
    "category_url = \"catalogue/category/books/mystery_3/index.html\"\n",
    "with s.get(shop_url + category_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/sharp-objects_997/index.html'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = response.find_all(\"article\", class_=\"product_pod\")\n",
    "# Extract the first link (which happens to be the picture), although we can get the second link too\n",
    "products[0].find('a').get('href')\n",
    "\n",
    "# as the full site per book is \n",
    "# https://books.toscrape.com/catalogue/scott-pilgrims-......html\n",
    "# we should thus strip ../../.. but keep the catalogue\n",
    "shop_url+'catalogue'+products[0].find('a').get('href')[8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! So we now know all the URLs per product\n",
    "\n",
    "### Finding the number of pages to go through\n",
    "\n",
    "Now we need to know how to iterate through pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32 results - showing 1 to 20.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is no 'number of pages' to check, but we can estimate the number from \n",
    "# the amount of results at the top left of the page. For instance:\n",
    "response.find(\"form\", class_=\"form-horizontal\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting with spaces and then extracting the first item in the list gets us the number by category\n",
    "number_of_products = response.find(\"form\", class_=\"form-horizontal\").text.strip().split(\" \")[0]\n",
    "#knowing that each page displays 20 products and rounding up gives us the number of pages\n",
    "round(int(number_of_products)/20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting this all together to scrape the whole category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html\n"
     ]
    }
   ],
   "source": [
    "# Scrape the main site URL + the category we want in the catagory of interest\n",
    "category_url = \"catalogue/category/books/historical-fiction_4/\"\n",
    "with s.get(shop_url + category_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# create empty list to save URLs of each product/book to scrape into a list\n",
    "products_to_scrape = []\n",
    "\n",
    "# Find the number of pages to iterate through:\n",
    "number_of_products = response.find(\"form\", class_=\"form-horizontal\").text.strip().split(\" \")[0]\n",
    "# iterate through the list of pages\n",
    "for page in range(0,math.ceil(int(number_of_products)/20)):\n",
    "    # as we start off scraping the category site anyway, then we don't need to scrape it again\n",
    "    # however if we are now on page 2, we haven't yet scraped it so we should\n",
    "    if page > 0:\n",
    "        category_url = category_url + \"page-{}.html\".format(page+1)\n",
    "        print('getting',shop_url+category_url)\n",
    "        with s.get(shop_url + category_url, headers=heads) as res:\n",
    "            response = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # Find all the product pods on this page\n",
    "    products = response.find_all(\"article\", class_=\"product_pod\")\n",
    "    # Extract the first link (which happens to be the picture), although we can get the second link too\n",
    "    for product in products:\n",
    "        href = product.find('a').get('href')\n",
    "        products_to_scrape.append(shop_url+'catalogue'+product.find('a').get('href')[8:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://books.toscrape.com/catalogue/the-house-by-the-lake_846/index.html'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 3 products\n",
    "products_to_scrape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check the number sraped\n",
    "len(products_to_scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As this page has 75 products/books - we scraped them all!\n",
    "\n",
    "## Scaping the individual product page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the main site URL + the category we want in the catagory of interest\n",
    "category_url = \"catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\"\n",
    "with s.get(shop_url + category_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get product (i.e. book) title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mesaerion: The Best Science Fiction Stories 1800-1849'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get product/book name\n",
    "response.title.text.split(\"|\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Product description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Barger, award-winning author and engineer, has extensively researched forgotten journals and magazines of the early 19th century to locate groundbreaking science fiction short stories in the English language. In doing so, he found what is possibly the first science fiction story by a female (and it is not from Mary Shelley). Andrew located the first steampunk short Andrew Barger, award-winning author and engineer, has extensively researched forgotten journals and magazines of the early 19th century to locate groundbreaking science fiction short stories in the English language. In doing so, he found what is possibly the first science fiction story by a female (and it is not from Mary Shelley). Andrew located the first steampunk short story, which has not been republished since 1844. There is the first voyage to the moon in a balloon, republished for the first time since 1820 that further tells of a darkness machine and a lunarian named Zuloc. Other sci-stories include the first robotic insect and an electricity gun. Once again, Andrew has searched old texts to find the very best science fiction stories from the period when the genre automated to life, some of the stories are published for the first time in nearly 200 years. Read these fantastic stories today!OUR OWN COUNTRY So mechanical has the age become, that men seriously talk of flying machines, to go by steam, --not your air-balloons, but real Daedalian wings, made of wood and joints, nailed to your shoulder, --not wings of feathers and wax like the wings of Icarus, who fell into the Cretan sea, but real, solid, substantial, rock-maple wings with wrought-iron hinges, and huge concavities, to propel us through the air. Knickerbocker Magazine, May 18 ...more\n"
     ]
    }
   ],
   "source": [
    "# description is challenging as it has no unique id or class, thus we could\n",
    "# find the product description tab (which is the first sub-header class) and\n",
    "# use the `find_next()` method to get to the description\n",
    "print(response.find_all(\"div\", class_=\"sub-header\")[0].find_next('p').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get product information\n",
    "\n",
    "As this is a table, there are 2 ways of getting this information, via `BeautifulSoup` and via `pandas`:\n",
    "\n",
    "**`BeautifulSoup` approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UPC': 'e30f54cea9b38190',\n",
       " 'Product Type': 'Books',\n",
       " 'Price (excl. tax)': 'Â£37.59',\n",
       " 'Price (incl. tax)': 'Â£37.59',\n",
       " 'Tax': 'Â£0.00',\n",
       " 'Availability': 'In stock (19 available)',\n",
       " 'Number of reviews': '0'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the table into a dictionary\n",
    "product_info = {}\n",
    "\n",
    "# filter to the product page\n",
    "product_page = response.find(\"article\", class_=\"product_page\")\n",
    "# as this is the only table with rows, we can just get find all rows on the product page\n",
    "rows = product_page.find_all('tr')\n",
    "for row in rows:\n",
    "    # focus on each row's cells and then save the values\n",
    "    cells = row.find_all(['th', 'td'])\n",
    "    product_info[cells[0].text] = cells[1].text\n",
    "    \n",
    "# have a look at what we saved\n",
    "product_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`pandas` approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UPC</td>\n",
       "      <td>e30f54cea9b38190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Type</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Price (excl. tax)</td>\n",
       "      <td>£37.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price (incl. tax)</td>\n",
       "      <td>£37.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tax</td>\n",
       "      <td>£0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Availability</td>\n",
       "      <td>In stock (19 available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of reviews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                        1\n",
       "0                UPC         e30f54cea9b38190\n",
       "1       Product Type                    Books\n",
       "2  Price (excl. tax)                   £37.59\n",
       "3  Price (incl. tax)                   £37.59\n",
       "4                Tax                    £0.00\n",
       "5       Availability  In stock (19 available)\n",
       "6  Number of reviews                        0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tables = pd.read_html(shop_url + category_url)\n",
    "all_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UPC': 'e30f54cea9b38190',\n",
       " 'Product Type': 'Books',\n",
       " 'Price (excl. tax)': '£37.59',\n",
       " 'Price (incl. tax)': '£37.59',\n",
       " 'Tax': '£0.00',\n",
       " 'Availability': 'In stock (19 available)',\n",
       " 'Number of reviews': '0'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can convert this to a dictionary by first setting the first \n",
    "# column as the index and second by telling pandas to convert it to a dictionary\n",
    "all_tables[0].set_index(0).to_dict()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: scraping the whole site\n",
    "\n",
    "Now that we've explored it all - we can put it together and create a scraper for the whole site!\n",
    "\n",
    "However instead of trying to do everything in one long script, lets at least break it out into two jobs:\n",
    "1. a script that checks the entire site and saves the URLs of each product that needs to be scraped, and\n",
    "2. a script that uses the URL of the product by getting all product characteristics that need saving - and this we already did above!\n",
    "\n",
    "Thus #1 is the main thing that we need to do - i.e. find all the product URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL\n",
    "shop_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Use the with clause we learned about (could also be done directly) to \n",
    "# collect and parse the site\n",
    "with s.get(shop_url, headers=heads) as res:\n",
    "    response = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# Get all categories\n",
    "side_category_section = response.find(\"div\", class_ = \"side_categories\")\n",
    "\n",
    "# Isolate all the categories using the link tag as they will have a link\n",
    "categories_found = side_category_section.find_all('a')\n",
    "\n",
    "# Iterate through all the categories and inside each through each page\n",
    "# to save the link of the product in a dictionary\n",
    "dictionary_of_products = {}\n",
    "\n",
    "for categories in categories_found:\n",
    "    # Since the 'Books' category is a catch-all, we want to skip it\n",
    "    if categories.text.strip() == 'Books':\n",
    "        continue\n",
    "\n",
    "    # For the category, create a dictionary to nest that will save the relevant info\n",
    "    dictionary_of_products[categories.text.strip()] = {}\n",
    "    \n",
    "    # Save the URL of the category within this nested dictionary\n",
    "    dictionary_of_products[categories.text.strip()]['category_url'] = categories.get('href')[:-10]\n",
    "\n",
    "    # break\n",
    "    # Initiate an empty list of urls to scrape for the category\n",
    "    products_to_scrape_list = []\n",
    "    for i, each_category in enumerate(categories):\n",
    "        # get the category page\n",
    "        with s.get(shop_url + categories.get('href')[:-10], headers=heads) as res:\n",
    "            response = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # Find the number of pages to iterate through:\n",
    "        number_of_products = response.find(\"form\", class_=\"form-horizontal\").text.strip().split(\" \")[0]\n",
    "        # iterate through the list of pages\n",
    "        for page in range(0,math.ceil(int(number_of_products)/20)):\n",
    "            # as we start off scraping the category site anyway, then we don't need to scrape it again\n",
    "            # however if we are now on page 2, we haven't yet scraped it so we should\n",
    "            if page > 0:\n",
    "                category_url = categories.get('href')[:-10] + \"page-{}.html\".format(page+1)\n",
    "                with s.get(shop_url + category_url, headers=heads) as res:\n",
    "                    response = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "            # Find all the product pods on this page\n",
    "            products = response.find_all(\"article\", class_=\"product_pod\")\n",
    "            # Extract the first link (which happens to be the picture), although we can get the second link too\n",
    "            for product in products:\n",
    "                href = product.find('a').get('href')\n",
    "                products_to_scrape_list.append(shop_url+'catalogue'+product.find('a').get('href')[8:])\n",
    "\n",
    "        # Save the nominal number of products and the list of urls in the main dictionary\n",
    "        dictionary_of_products[categories.text.strip()]['nominal_number_of_products'] = number_of_products\n",
    "        dictionary_of_products[categories.text.strip()]['products_to_scrape_list'] = products_to_scrape_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we've scraped the entire site - and saved the list of products into a list by category - in a dictionary called `dictionary_of_products`. Let's check if there were no mistakes and that we actually saved 1000 product URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a little counter - starting with zero\n",
    "counter = 0\n",
    "# go through each category\n",
    "for category in dictionary_of_products:\n",
    "    # and add to the counter the number of products\n",
    "    counter += len(dictionary_of_products[category]['products_to_scrape_list'])\n",
    "\n",
    "# now display the total\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category_url': 'catalogue/category/books/travel_2/',\n",
       " 'nominal_number_of_products': '11',\n",
       " 'products_to_scrape_list': ['https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html',\n",
       "  'https://books.toscrape.com/catalogue/full-moon-over-noahs-ark-an-odyssey-to-mount-ararat-and-beyond_811/index.html',\n",
       "  'https://books.toscrape.com/catalogue/see-america-a-celebration-of-our-national-parks-treasured-sites_732/index.html',\n",
       "  'https://books.toscrape.com/catalogue/vagabonding-an-uncommon-guide-to-the-art-of-long-term-world-travel_552/index.html',\n",
       "  'https://books.toscrape.com/catalogue/under-the-tuscan-sun_504/index.html',\n",
       "  'https://books.toscrape.com/catalogue/a-summer-in-europe_458/index.html',\n",
       "  'https://books.toscrape.com/catalogue/the-great-railway-bazaar_446/index.html',\n",
       "  'https://books.toscrape.com/catalogue/a-year-in-provence-provence-1_421/index.html',\n",
       "  'https://books.toscrape.com/catalogue/the-road-to-little-dribbling-adventures-of-an-american-in-britain-notes-from-a-small-island-2_277/index.html',\n",
       "  'https://books.toscrape.com/catalogue/neither-here-nor-there-travels-in-europe_198/index.html',\n",
       "  'https://books.toscrape.com/catalogue/1000-places-to-see-before-you-die_1/index.html']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see what it looks like for one category (as it would be really busy if we looked at a few)\n",
    "dictionary_of_products['Travel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, the total is 1000, so we got all product URLs! \n",
    "\n",
    "# Conclusion \n",
    "\n",
    "This notebook has demonstrated how to explore and parse the information on the site and get what we need. While we did not make a complex script that did everything all in one script, there is actually a very good reason -- this script would be big, hard to code, hard to understand, and hard to debug as we're likely to have mistakes. This would also make it a very RAP-unfriendly way to code. Thus we will see how the whole scraper can be developed to be more robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
