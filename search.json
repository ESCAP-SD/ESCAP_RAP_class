[
  {
    "objectID": "prices_scraper/notebooks/session11_code.html",
    "href": "prices_scraper/notebooks/session11_code.html",
    "title": "ESCAP training on RAP",
    "section": "",
    "text": "Aim: to scrape all product names and prices for women’s fashion from the Farmers.co.nz site\nStart with just one page - the first page for women’s tops.\n\nimport requests \nfrom bs4 import BeautifulSoup \nimport pandas as pd \n\n# Define the URL of the website\nurl = \"https://www.farmers.co.nz/women/fashion/tops\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the product listings\n    products = soup.find_all(\"div\", class_ = \"product-tile\")\n    \n    # Create lists to store the data\n    product_names = []\n    product_prices = []\n    \n    # Loop through the product listings and extract the data\n    for product in products:\n        name_tag = product.find(\"span\", class_ = \"product-title-span\") # found by inspecting html\n        price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n        \n        if name_tag:\n            name = name_tag.text.strip()  \n        else:\n            name = \"N/A\"\n        \n        if price_tag:\n            price = price_tag.text.strip()\n        else:\n            price = \"N/A\"\n        \n        product_names.append(name)\n        product_prices.append(price)\n    \n    # Create a DataFrame from the lists\n    df = pd.DataFrame({\n        \"Product Name\": product_names,\n        \"Product Price\": product_prices\n    })\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(\"farmers_women_tops_p1.csv\", index=False)\n    \n    print(\"Data has been written to farmers_women_tops_p1.csv\")\nelse:\n    print(\"Failed to retrieve the webpage.\")\n\nData has been written to farmers_women_tops_p1.csv\n\n\nThat got 24 prices (one page) - next step is to extend across all pages for women’s tops\nCall this new file farmers_women_tops.csv\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\nbase_url = \"https://www.farmers.co.nz/women/fashion/tops\"\n\n# Lists to store the product data\nproduct_names = []\nproduct_prices = []\n\n# Loop through each page (0 to 14 in this specific case) \nfor page_num in range(0, 15): # note, 2nd number of range not included, so needs to be 14+1\n    # Modify the URL to include the page number\n    url = f\"{base_url}/Page-{page_num}-SortingAttribute-SortBy-asc\" # specific to the Farmers.co.nz site\n    \n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        # Find the product listings\n        products = soup.find_all(\"div\", class_=\"product-tile\") # found by inspecting html\n        \n        # Loop through the product listings and extract the data\n        for product in products:\n            name_tag = product.find(\"span\", class_=\"product-title-span\") # found by inspecting html\n            price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n            \n            # Extract the product name\n            if name_tag:\n                name = name_tag.text.strip()  \n            else:\n                name = \"N/A\"\n            \n            # extract the product price\n            if price_tag:\n                price = price_tag.text.strip()\n            else:\n                price = \"N/A\"\n            \n            # Append the data to the lists\n            product_names.append(name)\n            product_prices.append(price)\n    else:\n        print(f\"Failed to retrieve page {page_num}\")\n\n# Create a DataFrame from the lists\ndf = pd.DataFrame({\n    \"Product Name\": product_names,\n    \"Product Price\": product_prices\n})\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"farmers_women_tops.csv\", index=False)\n\nprint(\"Data has been written to farmers_women_tops.csv\")\n\nData has been written to farmers_women_tops.csv\n\n\nSuccessfully scraped all 348 women’s tops.\nNext need to work out how to loop across categories (i.e. ‘new arrivals’, ‘dresses’, ‘tops’…). Work out how to get the URLs for each category.\n\n# Define the URL of the website\nurl = \"https://www.farmers.co.nz/women/fashion\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the categories\n    categories = soup.find_all(\"a\", class_ = \"category-list-image\")\n    \n    # Create list to store the data\n    category_urls = []\n\n    # Loop through the categories and extract the names\n    for category in categories:\n        url = category.get(\"href\", \"N/A\")\n        category_urls.append(url)\n          \n    print(\"category_urls list has been created\")\nelse:\n    print(\"Failed to retrieve the webpage.\")    \n\ncategory_urls list has been created\n\n\nWork out how to get the number of pages as a variable so it doesn’t have to be hard-coded as above for tops (which had pages 0-14)\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\nbase_url = \"https://www.farmers.co.nz/women/fashion/dresses\"\n\n# Send a GET request to the URL\nresponse = requests.get(base_url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the number of pages to be iterated through\n    pagenum_tag = soup.find_all(\"span\", class_ = \"pagination-hide\")\n\n    lastpage = pagenum_tag[-1].text.strip()  \n\n    lastpage_num = int(lastpage[2:])\n    print(\"retrieved number of pages\")\nelse:\n    print(\"Failed to retrieve the webpage.\")    \n\nretrieved number of pages\n\n\nNow try and combine the outer loop above with the code that scrapes all the pages (after determining number of pages) for each of the categories. And remember to add in a variable which has the date and time of the scrape.\n\n#import requests\n#from bs4 import BeautifulSoup\n#import pandas as pd\nfrom datetime import datetime # need this new module\n\n# Define the URL of the website\nurl = \"https://www.farmers.co.nz/women/fashion\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the categories\n    categories = soup.find_all(\"a\", class_ = \"category-list-image\")\n    \n    # Create list to store the data\n    category_urls = []\n\n    # Loop through the categories and extract the names\n    for category in categories:\n        url = category.get(\"href\", \"N/A\")\n        category_urls.append(url)\n          \n    print(\"category_urls list has been created\")\nelse:\n    print(\"Failed to retrieve the webpage.\")   \n\n# now run a loop across the list of categories (dresses, tops etc ) \n\n# Lists to store the product data\nproduct_names = []\nproduct_prices = []\nproduct_urls = [] # get the full url for now can transform at later stage\nscrape_times = [] \n    \nfor category_url in category_urls:\n    \n    # Base URL for the product category\n    base_url = category_url\n\n    # Send a GET request to the URL\n    response = requests.get(base_url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.content, \"html.parser\")\n    \n        # Find the number of pages to be iterated through\n        pagenum_tag = soup.find_all(\"span\", class_ = \"pagination-hide\")\n        if pagenum_tag == [] :\n            lastpage_num = 0\n        \n        else : \n\n            lastpage = pagenum_tag[-1].text.strip()  \n            lastpage_num = int(lastpage[2:])\n\n        print(f\"Number of pages is {lastpage_num}\")\n    else:\n        print(\"Failed to retrieve the webpage.\")    \n\n    for page_num in range(0, lastpage_num):\n        # Modify the URL to include the page number\n        url = f\"{base_url}/Page-{page_num}-SortingAttribute-SortBy-asc\"\n    \n        # Send a GET request to the URL\n        response = requests.get(url)\n    \n        # Check if the request was successful\n        if response.status_code == 200:\n            # Parse the HTML content using BeautifulSoup\n            soup = BeautifulSoup(response.content, \"html.parser\")\n        \n            # Find the product listings\n            products = soup.find_all(\"div\", class_=\"product-tile\")\n        \n            # Loop through the product listings and extract the data\n            for product in products:\n                name_tag = product.find(\"span\", class_=\"product-title-span\")\n                price_tag = product.find(\"div\", class_=\"current-price\")\n            \n                # Extract and clean the product name\n                name = name_tag.text.strip() if name_tag else \"N/A\"\n            \n                # Extract and clean the product price\n                price = price_tag.text.strip() if price_tag else \"N/A\"\n\n                # get time of scrape\n                scrape_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n            \n                # Append the data to the lists\n                product_names.append(name)\n                product_prices.append(price)\n                product_urls.append(base_url)\n                scrape_times.append(scrape_time)\n        else:\n            print(f\"Failed to retrieve page {page_num}\")\n\n        # Create a DataFrame from the lists\n    df = pd.DataFrame({\n        \"Product Name\": product_names,\n        \"Product Price\": product_prices,\n        \"Product Url\" : product_urls, # note this is actually the category URL (eg 'tops') so will help with categorisation\n        \"Scrape Time\" : scrape_times\n    })\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(\"farmers_womens_fashion.csv\", index=False)\n\n    print(f\"Data has been written to farmers_womens_fashion.csv for page {category_url}\")"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "Warning\n\n\n\nThis page is still under construction\n\n\n\n\nAs part of the Big Data Project, over the summer of 2024, ESCAP has been providing capacity support to project countries that have indicated web scraping for price statistics as a priority – by providing training on web scraping. Check out the rich training website for more details about the training in general, watch videos of past sessions, and review the materials.\nA key topic of this training is Reproducible Analytical Pipelines or RAP. RAP was originally developed in the United Kingdom government to improve the processes that government teams use to make their outputs more reproducible, as well as speed up their production processes. RAP is really a framework – that brings together a range of open source tools, techniques from fields like reproducible research, software engineering, and DevOps to make statistical releases easily reproducible, testable, and auditable (check out the original article from 2017explaining the concept and the purpose). Given that its a framework for mature processes, it can also be applied to the critical aspect of web scraping – as data collection and preparation for the CPI should be robust and highly reproducible!\nThe purpose of this course is thus to teach RAP to project countries and help them apply the concept to web scraping, but also help them practice the techniques and tools so that RAP can be applied to other pipelines to produce the CPI or other official statistics.\n\n\n\nThis site is used to structure and store the RAP training materials. Specifically, it aims to:\n\nSummarize how the training is structured and provide copies of the training slides\n\nCheck out the training scope;\nAs well as the slides for the September 11, 2024 session, as well as the September 18, 2024 session\n\nIt contains an example pipeline to scrape a website and create an output data file. This example acts as a demonstration of a production piece of software to demonstrate how jupyter notebooks that were heavily demonstrated in the virtual training are transformed into a more mature process that can be run regularly to support the CPI:\n\nThe site thus demonstrates various RAP principles in action in a simple way. This documentation is thus meant to show the key concept, but not necessarily be comprehensive.\nThe site also stores the code to operate the scraping RAP and documents its operational aspects\n\nThe site also provides a visual overview (via a process map) how a web scraper pipeline such as the one demonstrated can be integrated into other aspects of the CPI production process, commenting on how RAP can be applied to other aspects.\n\n\n\n\nThis site (and the training) does not aim to be comprehensive but simply demonstrate and document the key concepts. For more on RAP, we encourage you to check out:\n\nThe NHS RAP Communitity of Practice – that contains lots more useful content\nThe RAP Companion - which provides a good guide for each component (although focuses on R);\nThe Udemy class on Reproducible Analytical Pipelines (RAP) using R- provides a good overview\nA presentation and a paper providing a good guide of the application of RAP to price statistics, specifically for rail fares\nand many more!"
  },
  {
    "objectID": "docs/index.html#context",
    "href": "docs/index.html#context",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "As part of the Big Data Project, over the summer of 2024, ESCAP has been providing capacity support to project countries that have indicated web scraping for price statistics as a priority – by providing training on web scraping. Check out the rich training website for more details about the training in general, watch videos of past sessions, and review the materials.\nA key topic of this training is Reproducible Analytical Pipelines or RAP. RAP was originally developed in the United Kingdom government to improve the processes that government teams use to make their outputs more reproducible, as well as speed up their production processes. RAP is really a framework – that brings together a range of open source tools, techniques from fields like reproducible research, software engineering, and DevOps to make statistical releases easily reproducible, testable, and auditable (check out the original article from 2017explaining the concept and the purpose). Given that its a framework for mature processes, it can also be applied to the critical aspect of web scraping – as data collection and preparation for the CPI should be robust and highly reproducible!\nThe purpose of this course is thus to teach RAP to project countries and help them apply the concept to web scraping, but also help them practice the techniques and tools so that RAP can be applied to other pipelines to produce the CPI or other official statistics."
  },
  {
    "objectID": "docs/index.html#how-the-site-can-help-you-and-guide-to-navigating-it",
    "href": "docs/index.html#how-the-site-can-help-you-and-guide-to-navigating-it",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "This site is used to structure and store the RAP training materials. Specifically, it aims to:\n\nSummarize how the training is structured and provide copies of the training slides\n\nCheck out the training scope;\nAs well as the slides for the September 11, 2024 session, as well as the September 18, 2024 session\n\nIt contains an example pipeline to scrape a website and create an output data file. This example acts as a demonstration of a production piece of software to demonstrate how jupyter notebooks that were heavily demonstrated in the virtual training are transformed into a more mature process that can be run regularly to support the CPI:\n\nThe site thus demonstrates various RAP principles in action in a simple way. This documentation is thus meant to show the key concept, but not necessarily be comprehensive.\nThe site also stores the code to operate the scraping RAP and documents its operational aspects\n\nThe site also provides a visual overview (via a process map) how a web scraper pipeline such as the one demonstrated can be integrated into other aspects of the CPI production process, commenting on how RAP can be applied to other aspects."
  },
  {
    "objectID": "docs/index.html#useful-reading",
    "href": "docs/index.html#useful-reading",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "This site (and the training) does not aim to be comprehensive but simply demonstrate and document the key concepts. For more on RAP, we encourage you to check out:\n\nThe NHS RAP Communitity of Practice – that contains lots more useful content\nThe RAP Companion - which provides a good guide for each component (although focuses on R);\nThe Udemy class on Reproducible Analytical Pipelines (RAP) using R- provides a good overview\nA presentation and a paper providing a good guide of the application of RAP to price statistics, specifically for rail fares\nand many more!"
  },
  {
    "objectID": "docs/process-mapping.html",
    "href": "docs/process-mapping.html",
    "title": "Context of how web scraping integrates into a production CPI pipeline",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction"
  },
  {
    "objectID": "docs/teaching_materials/teaching.html",
    "href": "docs/teaching_materials/teaching.html",
    "title": "Documentation for the Reproducible Analytic Pipelines class held by ESCAP",
    "section": "",
    "text": "Note\n\n\n\nThis pages provides an overview of the course, but the actual material is still being finalized\n\n\nReproducible Analytic Pipelines (or RAP) is taught by UN ESCAP in two sessions. This training includes several components\n\n\nRAP is a large topic and it has its own Udemy course, supporting sites (such as the RAP companion), as well as communities of practice. ESCAP can’t thus hope to cover it in full, but instead aims to provide a detailed enough overview so that member countries being trained on web scraping for the CPI can benefit from maturity principles that RAP represents.\n\n\nThe overall objective of the RAP training is to provide an overview of key RAP principles, cover the value it brings to official statistics, as well as train class participants up to figure out just enough so that they can proceed on their own and help bring in RAP it into their own work. Class participants (who ESCAP trains) can thus become trainers in their own National Statistical Organizations (NSOs) - helping their teams and groups continue to learn more about RAP, explore each of the best practices, and learn how to implement it into their research or official statistics (i.e. production). In a more formal sense, this RAP class is thus aims to increase the capacity of participating NSOs to learn RAP and apply it for their continued modernization, and especially the implementation of alternative (web scrape most notably) data for consumer price statistics. Two sessions provide fist an overview and second a deeper dive on RAP\n\n\n\nObjective: Introduce ideas inherent in RAP and have participants walk away with an understanding why technical maturity skills are important and how RAP can help them mature their organization Topics to cover: * Why RAP is important to learn * What is RAP in more detail and what problem is it aiming to solve * Outline of key RAP principles and how they would help * Discussion on maturity levels - to show that its not all or nothing * Very brief demo of a use case to demonstrate something and walk away from the more theoretical ideas * Some caveats to keep in mind and wrap up for what you will see in person!\n\n\n\n\nObjective: Building on the initial training on RAP, go deeper into the components to really get the principles across. To help anchor the components, a use case can be used to understand how a little bit more about each aspect, and some hands-on exercises will help learners get a better appreciation of the tasks. Topics to cover: * Quick overview to recap the previous session * Deeper coverage of principles - weaving the use case into each component: * Automation * Modularity, re-usability, etc * Transparency * Open source tools * Version control * Good coding practices * Testing * Peer review * Exercise\nExercise in detail: To help anchor several of the technical skills in RAP, participants should spend about an hour in person (helped by the mentors who are there in person) try one (or two?) of a set of possible topics. Specifically, depending on their programming skills, access to a computer with Python, or knowledge of the overall process, participants can pick from several topics to spend an hour trying to implement. Possible topics: * Detailed mapping of the web scraping process - sketch out how your country’s specific web scraping needs to to be automated. Plan out the components and sub-components of the process to practice design and planning ahead of coding. * Develop unit tests - write a few scenarios for a specific task that would be a clearly encapsulated single function in the code. Then develop test data and create a unit test for this task * Partner up and try peer review - first develop a notebook (or script) to do some scraping. Then commit it to a github repository and then ask someone to peer review it using a separate branch (and a pull request) or simply an issue - to create transparency.\nFor all examples, some experience in python and setup of git with a github account is needed"
  },
  {
    "objectID": "docs/teaching_materials/teaching.html#scope-for-the-training",
    "href": "docs/teaching_materials/teaching.html#scope-for-the-training",
    "title": "Documentation for the Reproducible Analytic Pipelines class held by ESCAP",
    "section": "",
    "text": "RAP is a large topic and it has its own Udemy course, supporting sites (such as the RAP companion), as well as communities of practice. ESCAP can’t thus hope to cover it in full, but instead aims to provide a detailed enough overview so that member countries being trained on web scraping for the CPI can benefit from maturity principles that RAP represents.\n\n\nThe overall objective of the RAP training is to provide an overview of key RAP principles, cover the value it brings to official statistics, as well as train class participants up to figure out just enough so that they can proceed on their own and help bring in RAP it into their own work. Class participants (who ESCAP trains) can thus become trainers in their own National Statistical Organizations (NSOs) - helping their teams and groups continue to learn more about RAP, explore each of the best practices, and learn how to implement it into their research or official statistics (i.e. production). In a more formal sense, this RAP class is thus aims to increase the capacity of participating NSOs to learn RAP and apply it for their continued modernization, and especially the implementation of alternative (web scrape most notably) data for consumer price statistics. Two sessions provide fist an overview and second a deeper dive on RAP\n\n\n\nObjective: Introduce ideas inherent in RAP and have participants walk away with an understanding why technical maturity skills are important and how RAP can help them mature their organization Topics to cover: * Why RAP is important to learn * What is RAP in more detail and what problem is it aiming to solve * Outline of key RAP principles and how they would help * Discussion on maturity levels - to show that its not all or nothing * Very brief demo of a use case to demonstrate something and walk away from the more theoretical ideas * Some caveats to keep in mind and wrap up for what you will see in person!"
  },
  {
    "objectID": "docs/teaching_materials/teaching.html#rap-session-2",
    "href": "docs/teaching_materials/teaching.html#rap-session-2",
    "title": "Documentation for the Reproducible Analytic Pipelines class held by ESCAP",
    "section": "",
    "text": "Objective: Building on the initial training on RAP, go deeper into the components to really get the principles across. To help anchor the components, a use case can be used to understand how a little bit more about each aspect, and some hands-on exercises will help learners get a better appreciation of the tasks. Topics to cover: * Quick overview to recap the previous session * Deeper coverage of principles - weaving the use case into each component: * Automation * Modularity, re-usability, etc * Transparency * Open source tools * Version control * Good coding practices * Testing * Peer review * Exercise\nExercise in detail: To help anchor several of the technical skills in RAP, participants should spend about an hour in person (helped by the mentors who are there in person) try one (or two?) of a set of possible topics. Specifically, depending on their programming skills, access to a computer with Python, or knowledge of the overall process, participants can pick from several topics to spend an hour trying to implement. Possible topics: * Detailed mapping of the web scraping process - sketch out how your country’s specific web scraping needs to to be automated. Plan out the components and sub-components of the process to practice design and planning ahead of coding. * Develop unit tests - write a few scenarios for a specific task that would be a clearly encapsulated single function in the code. Then develop test data and create a unit test for this task * Partner up and try peer review - first develop a notebook (or script) to do some scraping. Then commit it to a github repository and then ask someone to peer review it using a separate branch (and a pull request) or simply an issue - to create transparency.\nFor all examples, some experience in python and setup of git with a github account is needed"
  },
  {
    "objectID": "docs/scraper_docs/scraper-docs.html",
    "href": "docs/scraper_docs/scraper-docs.html",
    "title": "Documentation about the demo scraper",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction"
  }
]