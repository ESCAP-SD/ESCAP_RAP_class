[
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "Warning\n\n\n\nThis page is still under construction\n\n\n\n\nAs part of the Big Data Project, over the summer of 2024, ESCAP has been providing capacity support to project countries that have indicated web scraping for price statistics as a priority – by providing training on web scraping. Check out the rich training website for more details about the training in general, watch videos of past sessions, and review the materials.\nA key topic of this training is Reproducible Analytical Pipelines or RAP. RAP was originally developed in the United Kingdom government to improve the processes that government teams use to make their outputs more reproducible, as well as speed up their production processes. RAP is really a framework – that brings together a range of open source tools, techniques from fields like reproducible research, software engineering, and DevOps to make statistical releases easily reproducible, testable, and auditable (check out the original article from 2017explaining the concept and the purpose). Given that its a framework for mature processes, it can also be applied to the critical aspect of web scraping – as data collection and preparation for the CPI should be robust and highly reproducible!\nThe purpose of this course is thus to teach RAP to project countries and help them apply the concept to web scraping, but also help them practice the techniques and tools so that RAP can be applied to other pipelines to produce the CPI or other official statistics.\n\n\n\nThis site is used to structure and store the RAP training materials. Specifically, it aims to:\n\nSummarize how the training is structured and provide copies of the training slides\n\nCheck out the training scope;\nAs well as the slides for the September 11, 2024 session, as well as the September 18, 2024 session\n\nIt contains an example pipeline to scrape a website and create an output data file. This example acts as a demonstration of a production piece of software to demonstrate how jupyter notebooks that were heavily demonstrated in the virtual training are transformed into a more mature process that can be run regularly to support the CPI:\n\nThe site thus demonstrates various RAP principles in action in a simple way. This documentation is thus meant to show the key concept, but not necessarily be comprehensive.\nThe site also stores the code to operate the scraping RAP and documents its operational aspects\n\nThe site also provides a visual overview (via a process map) how a web scraper pipeline such as the one demonstrated can be integrated into other aspects of the CPI production process, commenting on how RAP can be applied to other aspects.\n\n\n\n\nThis site (and the training) does not aim to be comprehensive but simply demonstrate and document the key concepts. For more on RAP, we encourage you to check out:\n\nThe NHS RAP Communitity of Practice – that contains lots more useful content\nThe RAP Companion - which provides a good guide for each component (although focuses on R);\nThe Udemy class on Reproducible Analytical Pipelines (RAP) using R- provides a good overview\nA presentation and a paper providing a good guide of the application of RAP to price statistics, specifically for rail fares\nand many more!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#context",
    "href": "docs/index.html#context",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "As part of the Big Data Project, over the summer of 2024, ESCAP has been providing capacity support to project countries that have indicated web scraping for price statistics as a priority – by providing training on web scraping. Check out the rich training website for more details about the training in general, watch videos of past sessions, and review the materials.\nA key topic of this training is Reproducible Analytical Pipelines or RAP. RAP was originally developed in the United Kingdom government to improve the processes that government teams use to make their outputs more reproducible, as well as speed up their production processes. RAP is really a framework – that brings together a range of open source tools, techniques from fields like reproducible research, software engineering, and DevOps to make statistical releases easily reproducible, testable, and auditable (check out the original article from 2017explaining the concept and the purpose). Given that its a framework for mature processes, it can also be applied to the critical aspect of web scraping – as data collection and preparation for the CPI should be robust and highly reproducible!\nThe purpose of this course is thus to teach RAP to project countries and help them apply the concept to web scraping, but also help them practice the techniques and tools so that RAP can be applied to other pipelines to produce the CPI or other official statistics.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#how-the-site-can-help-you-and-guide-to-navigating-it",
    "href": "docs/index.html#how-the-site-can-help-you-and-guide-to-navigating-it",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "This site is used to structure and store the RAP training materials. Specifically, it aims to:\n\nSummarize how the training is structured and provide copies of the training slides\n\nCheck out the training scope;\nAs well as the slides for the September 11, 2024 session, as well as the September 18, 2024 session\n\nIt contains an example pipeline to scrape a website and create an output data file. This example acts as a demonstration of a production piece of software to demonstrate how jupyter notebooks that were heavily demonstrated in the virtual training are transformed into a more mature process that can be run regularly to support the CPI:\n\nThe site thus demonstrates various RAP principles in action in a simple way. This documentation is thus meant to show the key concept, but not necessarily be comprehensive.\nThe site also stores the code to operate the scraping RAP and documents its operational aspects\n\nThe site also provides a visual overview (via a process map) how a web scraper pipeline such as the one demonstrated can be integrated into other aspects of the CPI production process, commenting on how RAP can be applied to other aspects.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#useful-reading",
    "href": "docs/index.html#useful-reading",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "This site (and the training) does not aim to be comprehensive but simply demonstrate and document the key concepts. For more on RAP, we encourage you to check out:\n\nThe NHS RAP Communitity of Practice – that contains lots more useful content\nThe RAP Companion - which provides a good guide for each component (although focuses on R);\nThe Udemy class on Reproducible Analytical Pipelines (RAP) using R- provides a good overview\nA presentation and a paper providing a good guide of the application of RAP to price statistics, specifically for rail fares\nand many more!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_18/sept_18_session.html",
    "href": "docs/teaching_materials/sept_18/sept_18_session.html",
    "title": "Session 2: Deeper dive into key components",
    "section": "",
    "text": "Note\n\n\n\nThis page will be updated ahead of the September 18th session\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 2: Deeper dive into key components"
    ]
  },
  {
    "objectID": "docs/applying_rap/web_scraping.html",
    "href": "docs/applying_rap/web_scraping.html",
    "title": "RAP for web scraping",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Applying RAP for price statistics",
      "RAP for web scraping"
    ]
  },
  {
    "objectID": "docs/scraper_docs.html",
    "href": "docs/scraper_docs.html",
    "title": "Demo RAP scraper documentation",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction",
    "crumbs": [
      "Demo RAP scraper documentation"
    ]
  },
  {
    "objectID": "docs/scraper_docs.html#baseline-rap---getting-the-fundamentals-right",
    "href": "docs/scraper_docs.html#baseline-rap---getting-the-fundamentals-right",
    "title": "Demo RAP scraper documentation",
    "section": "Baseline RAP - getting the fundamentals right",
    "text": "Baseline RAP - getting the fundamentals right\nIn order for a publication to be considered a reproducible analytical pipeline, it must at least meet all of the requirements of Baseline RAP:\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code (use NHS Open Source Policy section on Readmes as a guide.\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).",
    "crumbs": [
      "Demo RAP scraper documentation"
    ]
  },
  {
    "objectID": "docs/scraper_docs.html#silver-rap---implementing-best-practice",
    "href": "docs/scraper_docs.html#silver-rap---implementing-best-practice",
    "title": "Demo RAP scraper documentation",
    "section": "Silver RAP - implementing best practice",
    "text": "Silver RAP - implementing best practice\nMeeting all of the above requirements, plus:\n\nOutputs are produced by code with minimal manual intervention.\nCode is well-documented including user guidance, explanation of code structure & methodology and docstrings for functions.\nCode is well-organised following standard directory format.\nReusable functions and/or classes are used where appropriate.\nCode adheres to agreed coding standards (e.g PEP8, style guide for Pyspark).\nPipeline includes a testing framework (unit tests, back tests).\nRepository includes dependency information (e.g. requirements.txt, PipFile, environment.yml).\nLogs are automatically recorded by the pipeline to ensure outputs are as expected.\nData is handled and output in a Tidy data format.",
    "crumbs": [
      "Demo RAP scraper documentation"
    ]
  },
  {
    "objectID": "docs/scraper_docs/logging.html",
    "href": "docs/scraper_docs/logging.html",
    "title": "Logging",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Logging"
    ]
  },
  {
    "objectID": "docs/scraper_docs/mapping_the_process.html",
    "href": "docs/scraper_docs/mapping_the_process.html",
    "title": "Mapping the process",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Mapping the process"
    ]
  },
  {
    "objectID": "docs/scraper_docs/mapping_the_process.html#the-overall-process",
    "href": "docs/scraper_docs/mapping_the_process.html#the-overall-process",
    "title": "Mapping the process",
    "section": "The overall process",
    "text": "The overall process\nIf we start out with the main script that Frances showed in the 11th session, we end up with a large script at the end of the training session_11_notebook . However if we visually map out what is happening, we can start to figure out how to break this out.\n\n\n\nMapping the overall scrape process of farmers.co.nz\n\n\nWe see that the scraper went to the website multiple times but it is conceptually done in 3 steps (green boxes that show dashed lines to the website):\n\nto get categories\nto get page 1 of the where products are listed for that category.\nrepeat step b for all the other pages\n\nThe rest of it is processing, that either includes:\n\nthe logic to go from one step to the other (light blue);\nspecific processing steps such as cleaning or saving the data (dark blue).\n\nAt the end, we end up with a final file (purple) for that one individual scrape.",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Mapping the process"
    ]
  },
  {
    "objectID": "docs/scraper_docs/mapping_the_process.html#rapifying-this-task",
    "href": "docs/scraper_docs/mapping_the_process.html#rapifying-this-task",
    "title": "Mapping the process",
    "section": "RAPifying this task",
    "text": "RAPifying this task\nThe above approach works, but it is quite cumbersome for production - and this is where RAP can help us. As the main set is done in one large code block, if something happens, its hard to dive in to understand what’s happening and to fix it. In other words, its not easily reproducible without spending time to recreate the whole process. As a key principles of RAP is modularization, lets try to figure out how to separate this out into more logical and easily testable/functions.\nSome initial thoughts:\n\nA simple and easy one is the fact that we call the retailer website several times and we repeat the same code. While this is not overly complex, we can separate it out and centralize it as one function, as this allows us to make sure that (1) we never make a mistake writing the same code twice, and (2) lets us add extra robustness for this step. For instance we may want to have special error checks if something happens to know that if something happens, this is where the error occured (instead of in the big main function). Or we cold also add log entries for every call we make to the retailer site in order to have an audit of all the external calls our package makes.\n\nWill thus make a new get_site_data(input_url) function that will centralize this.\n\nLooking further, we see that our main code is mostly logic, with a bunch of parsing (i.e. cleaning) functions (like extracting specific information from BeautifulSoup tags). If we separate out this parsing into separate functions and leave the logical structure in one main script, we (1) could easily see if one of the parsing functions failed, and (2) more easily understand and read, fix, or enhance the main logic. This is a classic example of loose coupling, good coding practices, and easier integration of tests for each function - all of which RAP recommends. Thus we can make a few custom functions:\n\nget_and_parse_categories()\nparse_last_page_num()\nparse_product_info() - this could be a main function that extracts the logic of extracting product info fo reasier extensibility, although for what we’re demoing, this is optional. This function would actually call several sub-functions:\n\nparse_product_name()\nparse_price()\nparse_product_url()\n\nsave_data() - again, separated for easier extensibility, but not strictly necessary.\n\nFinally, while we’ve moved the parsing and calls to the retailer site outside of the main logic, we should stick to the functional paradigm of programming by turning the main logic into a function - our main function!\n\n\n\n\nRAPified version that is still automatic, but with all the main components that cold fail separated. The functions act as building blocks for the logic, making it easy to undertand, debug, and extend",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Mapping the process"
    ]
  },
  {
    "objectID": "docs/scraper_docs/error_handling.html",
    "href": "docs/scraper_docs/error_handling.html",
    "title": "Error handling",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Error handling"
    ]
  },
  {
    "objectID": "notebooks/session11_code.html",
    "href": "notebooks/session11_code.html",
    "title": "Web scraper walkthrough: review from Session 11",
    "section": "",
    "text": "This page renders the jupyter notebook that Frances put together on August 21 to scrape (Farmers.co.nz).\n\nAim: to scrape all product names and prices for women’s fashion from the Farmers.co.nz site\n\nAs usual, one first need some packages to be loaded\n\nimport requests \nfrom bs4 import BeautifulSoup \nimport pandas as pd \n\n\n\nStart with just one page - the first page for women’s tops.\n\n\n\nA common first step is to confirm whether we can scrape at all - so we should check out the robots.txt.\nUser-agent:*\nDisallow: /cms/post-surgery-bra-fitting\nDisallow: /cms/mothers-day-v2\nDisallow: /women/new-collection/sally-ann-mullin-s-top-picks\nDisallow: /cms/comp\nDisallow: /cms/testcataloguetest\nDisallow: /cms/page_SelectorTest_20190823084433\nDisallow: /cms/instore-bed-selector\nDisallow: /cms/stevens-privacy-policy\nDisallow: /cms/sremraf\nDisallow: /cms/store-update\nDisallow: /cms/sleepyhead-early-bird-voucher\nDisallow: *SearchTerm=*\nDisallow: /filter/*\nDisallow: */INTERSHOP/web*\nDisallow: */ManufacturerName-*\nDisallow: */ProductSalePriceGross-*\nDisallow: */Size-*\nDisallow: */FilterClearance-*\nDisallow: */ColourFamilyDisplayName-*\nDisallow: */function-*\nDisallow: */gender-*\nDisallow: */age-*\nDisallow: */SpecialOffer-*\nDisallow: */StockStatus-*\nDisallow: */styleshape-*\nDisallow: */fabric-*\nDisallow: */fillingtype-*\nDisallow: */GroupSize*\nDisallow: *ContextCategoryUUID*\n\nSitemap: https://www.farmers.co.nz/sitemap-sitemap\n\nUser-agent: bingbot\nCrawl-delay: 1\nAs the site we want to scrape is https://www.farmers.co.nz/women/fashion/tops, this is not disallowed! However we see that there is a specific women’s category that we need to skip (/women/new-collection/sally-ann-mullin-s-top-picks), and the crawl-delay should be set to at least 1.\n\n\nIt is good to have a look at the website beforehand and anvigate a bit to see if the strucure looks easy to naviage, fand formating consistent\n\n# Define the URL of the website\nurl = \"https://www.farmers.co.nz/women/fashion/tops\"\n\n\n\n\nThen the URL of the website has to be tested. We send a request to the web server hosting the URL, asking for the content of the page.\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\nDepending on the response, we can start scrapping (or not). The responses can be: * 200 OK, there are some elements (data in return of our request) * 404: Not Found, nothing is returned, there may be an error in the URL * other responses such as 500: Internal Server Error, or 403 forbidden to acces…\nHere the sesponse is:\n\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nA code response = 200, means that the URL is correctly responding our request. We can then move on to scrap ;-)\n\n\n\n\nThe whole code will be embedded into an if then else structure:\nif one can scrap (then) scrap website (many actions there) at the end print a message with the location of the file with data else do nothing print a message informaing that nothing was retuned\n\n\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the product listings\n    products = soup.find_all(\"div\", class_ = \"product-tile\")\n    \n    # Create lists to store the data\n    product_names = []\n    product_prices = []\n    \n    # Loop through the product listings and extract the data\n    for product in products:\n        name_tag = product.find(\"span\", class_ = \"product-title-span\") # found by inspecting html\n        price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n        \n        if name_tag:\n            name = name_tag.text.strip()  \n        else:\n            name = \"N/A\"\n        \n        if price_tag:\n            price = price_tag.text.strip()\n        else:\n            price = \"N/A\"\n        \n        product_names.append(name)\n        product_prices.append(price)\n    \n    # Create a DataFrame from the lists\n    df = pd.DataFrame({\n        \"Product Name\": product_names,\n        \"Product Price\": product_prices\n    })\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(\"../data/raw/farmers_women_tops_p1.csv\", index=False)\n    \n    print(\"Data has been written to farmers_women_tops_p1.csv\")\nelse:\n    print(\"Failed to retrieve the webpage.\")\n\nData has been written to farmers_women_tops_p1.csv\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8 entries, 0 to 7\nData columns (total 2 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   Product Name   8 non-null      object\n 1   Product Price  8 non-null      object\ndtypes: object(2)\nmemory usage: 256.0+ bytes\n\n\nThat got 8 prices (one page) - next step is to extend across all pages for women’s tops\nCall this new file farmers_women_tops.csv\n\n\n\n\nBy looking at the resulting data frame we observe that the variable ProdcutPrice has some text in it.\nWe can use regular expressions to create a “Price” variable\n\n# Extracting the price using regex\ndf['Price'] = df['Product Price'].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n\n\n\nWe may be interested in a first overview of the data collected\n\n# Descriptive statistics for the Price variable\nprice_stats = df['Price'].describe()\n\n## Select the statistics we are interested in and display horizontally\nselected_stats = price_stats[[ 'min','mean', '50%' , 'max', 'count']].to_frame().T\n\nprint(selected_stats)\n\n         min   mean    50%    max  count\nPrice  49.99  67.49  69.99  79.99   24.0\n\n\n\n\n\n\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\nbase_url = \"https://www.farmers.co.nz/women/fashion/tops\"\n\n# Lists to store the product data\nproduct_names = []\nproduct_prices = []\n\n# Loop through each page (0 to 14 in this specific case) \nfor page_num in range(0, 15): # note, 2nd number of range not included, so needs to be 14+1\n    # Modify the URL to include the page number\n    url = f\"{base_url}/Page-{page_num}-SortingAttribute-SortBy-asc\" # specific to the Farmers.co.nz site\n    \n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        # Find the product listings\n        products = soup.find_all(\"div\", class_=\"product-tile\") # found by inspecting html\n        \n        # Loop through the product listings and extract the data\n        for product in products:\n            name_tag = product.find(\"span\", class_=\"product-title-span\") # found by inspecting html\n            price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n            \n            # Extract the product name\n            if name_tag:\n                name = name_tag.text.strip()  \n            else:\n                name = \"N/A\"\n            \n            # extract the product price\n            if price_tag:\n                price = price_tag.text.strip()\n            else:\n                price = \"N/A\"\n            \n            # Append the data to the lists\n            product_names.append(name)\n            product_prices.append(price)\n    else:\n        print(f\"Failed to retrieve page {page_num}\")\n\n# Create a DataFrame from the lists\ndf = pd.DataFrame({\n    \"Product Name\": product_names,\n    \"Product Price\": product_prices\n})\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"../data/raw/farmers_women_tops.csv\", index=False)\n\nprint(\"Data has been written to farmers_women_tops.csv\")\n\nData has been written to farmers_women_tops.csv\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 360 entries, 0 to 359\nData columns (total 2 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   Product Name   360 non-null    object\n 1   Product Price  360 non-null    object\ndtypes: object(2)\nmemory usage: 5.8+ KB\n\n\nSuccessfully scraped all 360 women’s tops.\nNext need to work out how to loop across categories (i.e. ‘new arrivals’, ‘dresses’, ‘tops’…). Work out how to get the URLs for each category.\n\n# Define the URL of the website\nurl = \"https://www.farmers.co.nz/women/fashion\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the categories\n    categories = soup.find_all(\"a\", class_ = \"category-list-image\")\n    \n    # Create list to store the data\n    category_urls = []\n\n    # Loop through the categories and extract the names\n    for category in categories:\n        url = category.get(\"href\", \"N/A\")\n        category_urls.append(url)\n          \n    print(\"category_urls list has been created\")\nelse:\n    print(\"Failed to retrieve the webpage.\")    \n\ncategory_urls list has been created\n\n\n\ncategory_urls\n\n['http://www.farmers.co.nz/women/fashion/new-arrivals',\n 'http://www.farmers.co.nz/women/fashion/dresses',\n 'http://www.farmers.co.nz/women/fashion/tops',\n 'http://www.farmers.co.nz/women/fashion/skirts',\n 'http://www.farmers.co.nz/women/fashion/jeans',\n 'http://www.farmers.co.nz/women/fashion/pants-leggings',\n 'http://www.farmers.co.nz/women/fashion/activewear',\n 'http://www.farmers.co.nz/women/fashion/shorts',\n 'http://www.farmers.co.nz/women/fashion/swimwear',\n 'http://www.farmers.co.nz/women/fashion/sweatshirts-hoodies',\n 'http://www.farmers.co.nz/women/fashion/knitwear',\n 'http://www.farmers.co.nz/women/fashion/coats-jackets']\n\n\nWork out how to get the number of pages as a variable so it doesn’t have to be hard-coded as above for tops (which had pages 0-14)\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\nbase_url = \"https://www.farmers.co.nz/women/fashion/dresses\"\n\n# Send a GET request to the URL\nresponse = requests.get(base_url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the number of pages to be iterated through\n    pagenum_tag = soup.find_all(\"span\", class_ = \"pagination-hide\")\n\n    lastpage = pagenum_tag[-1].text.strip()  \n\n    lastpage_num = int(lastpage[2:])\n    print(\"retrieved number of pages\")\nelse:\n    print(\"Failed to retrieve the webpage.\")    \n\nretrieved number of pages",
    "crumbs": [
      "Web scraper walkthrough: review from Session 11"
    ]
  },
  {
    "objectID": "notebooks/session11_code.html#initial-scrape-only-one-page",
    "href": "notebooks/session11_code.html#initial-scrape-only-one-page",
    "title": "Web scraper walkthrough: review from Session 11",
    "section": "",
    "text": "Start with just one page - the first page for women’s tops.",
    "crumbs": [
      "Web scraper walkthrough: review from Session 11"
    ]
  },
  {
    "objectID": "notebooks/session11_code.html#first-check-that-we-can",
    "href": "notebooks/session11_code.html#first-check-that-we-can",
    "title": "Web scraper walkthrough: review from Session 11",
    "section": "",
    "text": "A common first step is to confirm whether we can scrape at all - so we should check out the robots.txt.\nUser-agent:*\nDisallow: /cms/post-surgery-bra-fitting\nDisallow: /cms/mothers-day-v2\nDisallow: /women/new-collection/sally-ann-mullin-s-top-picks\nDisallow: /cms/comp\nDisallow: /cms/testcataloguetest\nDisallow: /cms/page_SelectorTest_20190823084433\nDisallow: /cms/instore-bed-selector\nDisallow: /cms/stevens-privacy-policy\nDisallow: /cms/sremraf\nDisallow: /cms/store-update\nDisallow: /cms/sleepyhead-early-bird-voucher\nDisallow: *SearchTerm=*\nDisallow: /filter/*\nDisallow: */INTERSHOP/web*\nDisallow: */ManufacturerName-*\nDisallow: */ProductSalePriceGross-*\nDisallow: */Size-*\nDisallow: */FilterClearance-*\nDisallow: */ColourFamilyDisplayName-*\nDisallow: */function-*\nDisallow: */gender-*\nDisallow: */age-*\nDisallow: */SpecialOffer-*\nDisallow: */StockStatus-*\nDisallow: */styleshape-*\nDisallow: */fabric-*\nDisallow: */fillingtype-*\nDisallow: */GroupSize*\nDisallow: *ContextCategoryUUID*\n\nSitemap: https://www.farmers.co.nz/sitemap-sitemap\n\nUser-agent: bingbot\nCrawl-delay: 1\nAs the site we want to scrape is https://www.farmers.co.nz/women/fashion/tops, this is not disallowed! However we see that there is a specific women’s category that we need to skip (/women/new-collection/sally-ann-mullin-s-top-picks), and the crawl-delay should be set to at least 1.\n\n\nIt is good to have a look at the website beforehand and anvigate a bit to see if the strucure looks easy to naviage, fand formating consistent\n\n# Define the URL of the website\nurl = \"https://www.farmers.co.nz/women/fashion/tops\"\n\n\n\n\nThen the URL of the website has to be tested. We send a request to the web server hosting the URL, asking for the content of the page.\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\nDepending on the response, we can start scrapping (or not). The responses can be: * 200 OK, there are some elements (data in return of our request) * 404: Not Found, nothing is returned, there may be an error in the URL * other responses such as 500: Internal Server Error, or 403 forbidden to acces…\nHere the sesponse is:\n\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nA code response = 200, means that the URL is correctly responding our request. We can then move on to scrap ;-)\n\n\n\n\nThe whole code will be embedded into an if then else structure:\nif one can scrap (then) scrap website (many actions there) at the end print a message with the location of the file with data else do nothing print a message informaing that nothing was retuned\n\n\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the product listings\n    products = soup.find_all(\"div\", class_ = \"product-tile\")\n    \n    # Create lists to store the data\n    product_names = []\n    product_prices = []\n    \n    # Loop through the product listings and extract the data\n    for product in products:\n        name_tag = product.find(\"span\", class_ = \"product-title-span\") # found by inspecting html\n        price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n        \n        if name_tag:\n            name = name_tag.text.strip()  \n        else:\n            name = \"N/A\"\n        \n        if price_tag:\n            price = price_tag.text.strip()\n        else:\n            price = \"N/A\"\n        \n        product_names.append(name)\n        product_prices.append(price)\n    \n    # Create a DataFrame from the lists\n    df = pd.DataFrame({\n        \"Product Name\": product_names,\n        \"Product Price\": product_prices\n    })\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(\"../data/raw/farmers_women_tops_p1.csv\", index=False)\n    \n    print(\"Data has been written to farmers_women_tops_p1.csv\")\nelse:\n    print(\"Failed to retrieve the webpage.\")\n\nData has been written to farmers_women_tops_p1.csv\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8 entries, 0 to 7\nData columns (total 2 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   Product Name   8 non-null      object\n 1   Product Price  8 non-null      object\ndtypes: object(2)\nmemory usage: 256.0+ bytes\n\n\nThat got 8 prices (one page) - next step is to extend across all pages for women’s tops\nCall this new file farmers_women_tops.csv",
    "crumbs": [
      "Web scraper walkthrough: review from Session 11"
    ]
  },
  {
    "objectID": "notebooks/session11_code.html#cleaning-the-web-scraped-data-frame",
    "href": "notebooks/session11_code.html#cleaning-the-web-scraped-data-frame",
    "title": "Web scraper walkthrough: review from Session 11",
    "section": "",
    "text": "By looking at the resulting data frame we observe that the variable ProdcutPrice has some text in it.\nWe can use regular expressions to create a “Price” variable\n\n# Extracting the price using regex\ndf['Price'] = df['Product Price'].str.extract(r'(\\d+\\.\\d+|\\d+)').astype(float)\n\n\n\nWe may be interested in a first overview of the data collected\n\n# Descriptive statistics for the Price variable\nprice_stats = df['Price'].describe()\n\n## Select the statistics we are interested in and display horizontally\nselected_stats = price_stats[[ 'min','mean', '50%' , 'max', 'count']].to_frame().T\n\nprint(selected_stats)\n\n         min   mean    50%    max  count\nPrice  49.99  67.49  69.99  79.99   24.0",
    "crumbs": [
      "Web scraper walkthrough: review from Session 11"
    ]
  },
  {
    "objectID": "notebooks/session11_code.html#second-part-looping-over-pages",
    "href": "notebooks/session11_code.html#second-part-looping-over-pages",
    "title": "Web scraper walkthrough: review from Session 11",
    "section": "",
    "text": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\nbase_url = \"https://www.farmers.co.nz/women/fashion/tops\"\n\n# Lists to store the product data\nproduct_names = []\nproduct_prices = []\n\n# Loop through each page (0 to 14 in this specific case) \nfor page_num in range(0, 15): # note, 2nd number of range not included, so needs to be 14+1\n    # Modify the URL to include the page number\n    url = f\"{base_url}/Page-{page_num}-SortingAttribute-SortBy-asc\" # specific to the Farmers.co.nz site\n    \n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        # Find the product listings\n        products = soup.find_all(\"div\", class_=\"product-tile\") # found by inspecting html\n        \n        # Loop through the product listings and extract the data\n        for product in products:\n            name_tag = product.find(\"span\", class_=\"product-title-span\") # found by inspecting html\n            price_tag = product.find(\"div\", class_=\"current-price\") # found by inspecting html\n            \n            # Extract the product name\n            if name_tag:\n                name = name_tag.text.strip()  \n            else:\n                name = \"N/A\"\n            \n            # extract the product price\n            if price_tag:\n                price = price_tag.text.strip()\n            else:\n                price = \"N/A\"\n            \n            # Append the data to the lists\n            product_names.append(name)\n            product_prices.append(price)\n    else:\n        print(f\"Failed to retrieve page {page_num}\")\n\n# Create a DataFrame from the lists\ndf = pd.DataFrame({\n    \"Product Name\": product_names,\n    \"Product Price\": product_prices\n})\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"../data/raw/farmers_women_tops.csv\", index=False)\n\nprint(\"Data has been written to farmers_women_tops.csv\")\n\nData has been written to farmers_women_tops.csv\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 360 entries, 0 to 359\nData columns (total 2 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   Product Name   360 non-null    object\n 1   Product Price  360 non-null    object\ndtypes: object(2)\nmemory usage: 5.8+ KB\n\n\nSuccessfully scraped all 360 women’s tops.\nNext need to work out how to loop across categories (i.e. ‘new arrivals’, ‘dresses’, ‘tops’…). Work out how to get the URLs for each category.\n\n# Define the URL of the website\nurl = \"https://www.farmers.co.nz/women/fashion\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the categories\n    categories = soup.find_all(\"a\", class_ = \"category-list-image\")\n    \n    # Create list to store the data\n    category_urls = []\n\n    # Loop through the categories and extract the names\n    for category in categories:\n        url = category.get(\"href\", \"N/A\")\n        category_urls.append(url)\n          \n    print(\"category_urls list has been created\")\nelse:\n    print(\"Failed to retrieve the webpage.\")    \n\ncategory_urls list has been created\n\n\n\ncategory_urls\n\n['http://www.farmers.co.nz/women/fashion/new-arrivals',\n 'http://www.farmers.co.nz/women/fashion/dresses',\n 'http://www.farmers.co.nz/women/fashion/tops',\n 'http://www.farmers.co.nz/women/fashion/skirts',\n 'http://www.farmers.co.nz/women/fashion/jeans',\n 'http://www.farmers.co.nz/women/fashion/pants-leggings',\n 'http://www.farmers.co.nz/women/fashion/activewear',\n 'http://www.farmers.co.nz/women/fashion/shorts',\n 'http://www.farmers.co.nz/women/fashion/swimwear',\n 'http://www.farmers.co.nz/women/fashion/sweatshirts-hoodies',\n 'http://www.farmers.co.nz/women/fashion/knitwear',\n 'http://www.farmers.co.nz/women/fashion/coats-jackets']\n\n\nWork out how to get the number of pages as a variable so it doesn’t have to be hard-coded as above for tops (which had pages 0-14)\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\nbase_url = \"https://www.farmers.co.nz/women/fashion/dresses\"\n\n# Send a GET request to the URL\nresponse = requests.get(base_url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find the number of pages to be iterated through\n    pagenum_tag = soup.find_all(\"span\", class_ = \"pagination-hide\")\n\n    lastpage = pagenum_tag[-1].text.strip()  \n\n    lastpage_num = int(lastpage[2:])\n    print(\"retrieved number of pages\")\nelse:\n    print(\"Failed to retrieve the webpage.\")    \n\nretrieved number of pages",
    "crumbs": [
      "Web scraper walkthrough: review from Session 11"
    ]
  },
  {
    "objectID": "docs/scraper_docs/modularity.html",
    "href": "docs/scraper_docs/modularity.html",
    "title": "Overview of modules",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Overview of modules"
    ]
  },
  {
    "objectID": "docs/scraper_docs/output_data.html",
    "href": "docs/scraper_docs/output_data.html",
    "title": "ESCAP training on RAP",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Output Data"
    ]
  },
  {
    "objectID": "docs/404.html",
    "href": "docs/404.html",
    "title": "Page Not Found",
    "section": "",
    "text": "The page you requested cannot be found (perhaps it was moved or renamed).\nGo back to the home page or search for what you are looking for\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html",
    "href": "docs/applying_rap/process-mapping.html",
    "title": "Applying RAP for price statistics",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html#overview-of-the-ads-processing-view",
    "href": "docs/applying_rap/process-mapping.html#overview-of-the-ads-processing-view",
    "title": "Applying RAP for price statistics",
    "section": "Overview of the ADS processing view",
    "text": "Overview of the ADS processing view\nWeb scraping is the first of several phases necessary to create elementary aggregates –- which are the main input and the building blocks of the Consumer Prices Index. The below sections first provide an overview of the key step and options of how RAP cap apply to the overall process, then go through each step in order to gather detailed requirements for the web scraping step.\n\nOverview of all the steps\nThe below diagram provides an overview of 4 main steps that include the web scraping step, data processing or preparation step, the classification step, and the price index or aggregation step. Each step outputs a dataset that is used as an input for the next step.\n\n\n\n\n\n\nFigure 1: High level overview of the steps to process web scrape data for the CPI\n\n\n\nEach step is made up of one or more sub-components:\n\nThe web scraping step contains the scraping aspect itself,1 but also includes dataset validation that will help make sure that the scraper is operating as it is expected, as well as reports for review;2\nThe data processing step contains the process to standardizing and preparation step.3\nThe classification step contains several sub-steps, such as identifying unique products to classify, the classification method itself, and the manual validation of classification (in cases there are errors).4\nFor the price index step, many sub-steps are involved.5\n\n\n\nHow does RAP come into this?\nGiven this context, RAP can be applied in several ways. As RAP is a way to encapsulate the creation of a statistical (in our case) process into one corpus (i.e. repository with all contexts) – we can:\n\nEncapsulate the whole process in one repository. As RAP is meant to help minimize coupling (dependencies) between separate processes – treating the whole process end-to-end as one RAP is useful if no steps in this pipeline have to be shared with other pipelines.\nEncapsulate each step into a separate RAP. This may be appropriate if several processing phases are done in sequence with specific targeted stopping points – such as handoff between teams or if some steps are shared or where manual steps are necessary. The UK RAP implementation shows how several pipelines operate on top of a data architecture in sequence.6\nEncapsulate as appropriate. Several steps could be combined to simplify the process and if separation is not required, such as the web scraping and data processing steps.\n\nFor this guide and for the demo RAP scraper, we will follow approach (b) as this will allow us to develop RAP for just the scraping component.",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html#deep-dive-into-each-component",
    "href": "docs/applying_rap/process-mapping.html#deep-dive-into-each-component",
    "title": "Applying RAP for price statistics",
    "section": "Deep dive into each component",
    "text": "Deep dive into each component\nCheck out the next sections to see how to to put this into practice.",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html#footnotes",
    "href": "docs/applying_rap/process-mapping.html#footnotes",
    "title": "Applying RAP for price statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Practical guidelines on web scraping for the HICP (2020), specifically Annex I and VII for mor↩︎\nSee Monitoring, validation and plausibility checks for web scraped data (UN e-Handbook) for more details↩︎\nSee Preparation of data (UN e-Handbook) for more handbook.↩︎\nFor an overview of Classification process in production and other operational aspects, see “Classification of Alternative Data Sources”, 2024-05-14. For an overview of the 5 main classification methods, see “Classifying Alterantive Data Sources for Consumer Prices Statistics: Methods and best practices”, 2023-06-08.↩︎\nSee “Practical guidelines on web scraping for the HICP (2020)” for more details↩︎\nSee Price (2023) Developing reproducible analytical pipelines for the transformation of consumer price statistics: rail fares for more details, for instance 5.2 outlines Pipeline tables and how different pipelines interact over a specific data architecture.↩︎",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/teaching_materials/teaching.html",
    "href": "docs/teaching_materials/teaching.html",
    "title": "ESCAP sesions on RAP",
    "section": "",
    "text": "Note\n\n\n\nThis pages provides an overview of the course, but the actual material is still being finalized\n\n\nReproducible Analytic Pipelines (or RAP) is taught by UN ESCAP in two sessions. This training includes several components\n\nOverall scope of the training\nRAP is a large topic and it has its own Udemy course, supporting sites (such as the RAP companion), as well as communities of practice. ESCAP can’t thus hope to cover it in full, but instead aims to provide a detailed enough overview so that member countries being trained on web scraping for the CPI can benefit from maturity principles that RAP represents.\nOverall objective\nThe overall objective of the RAP training is to provide an overview of key RAP principles, cover the value it brings to official statistics, as well as train class participants up to figure out just enough so that they can proceed on their own and help bring in RAP it into their own work. Class participants (who ESCAP trains) can thus become trainers in their own National Statistical Organizations (NSOs) - helping their teams and groups continue to learn more about RAP, explore each of the best practices, and learn how to implement it into their research or official statistics (i.e. production). In a more formal sense, this RAP class is thus aims to increase the capacity of participating NSOs to learn RAP and apply it for their continued modernization, and especially the implementation of alternative (web scrape most notably) data for consumer price statistics. Two sessions provide fist an overview and second a deeper dive on RAP\n\n\nSession 1, virtual: Overview of RAP\nDate: September 11, 2024\nObjective: Introduce ideas inherent in RAP and have participants walk away with an understanding why technical maturity skills are important and how RAP can help them mature their organization\nTopics to cover:\n\nWhy RAP is important to learn\nWhat is RAP in more detail and what problem is it aiming to solve\nOutline of key RAP principles and how they would help\nDiscussion on maturity levels - to show that its not all or nothing\nVery brief demo of a use case to demonstrate something and walk away from the more theoretical ideas\nSome caveats to keep in mind and wrap up for what you will see in person!\n\nContent: See the session page for more details.\n\n\nSession 2, in person: Deeper dive into RAP components\nDate: September 18, 2024\nObjective: Building on the initial training on RAP, go deeper into the components to really get the principles across. To help anchor the components, a use case can be used to understand how a little bit more about each aspect, and some hands-on exercises will help learners get a better appreciation of the tasks.\nTopics to cover:\n\nQuick overview to recap the previous session\nDeeper coverage of principles - weaving the use case into each component:\n\nAutomation\nModularity, re-usability, etc\nTransparency\nOpen source tools\nVersion control\nGood coding practices\nTesting\nPeer review\n\nExercise\n\nExercise in detail: To help anchor several of the technical skills in RAP, participants should spend about an hour in person (helped by the mentors who are there in person) try one (or two?) of a set of possible topics. Specifically, depending on their programming skills, access to a computer with Python, or knowledge of the overall process, participants can pick from several topics to spend an hour trying to implement. Possible topics:\n\nDetailed mapping of the web scraping process - sketch out how your country’s specific web scraping needs to to be automated. Plan out the components and sub-components of the process to practice design and planning ahead of coding.\nDevelop unit tests - write a few scenarios for a specific task that would be a clearly encapsulated single function in the code. Then develop test data and create a unit test for this task\nPartner up and try peer review - first develop a notebook (or script) to do some scraping. Then commit it to a github repository and then ask someone to peer review it using a separate branch (and a pull request) or simply an issue - to create transparency.\n\nFor all examples, some experience in python and setup of git with a github account is needed\nContent: See the session page for more details.\n\n\n\n\n Back to top",
    "crumbs": [
      "ESCAP sesions on RAP"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_11/sept_11_session.html",
    "href": "docs/teaching_materials/sept_11/sept_11_session.html",
    "title": "Session 1: Introduction to RAP",
    "section": "",
    "text": "Overview\n\n\nPresentation for the session\n\n\n\nPresentation on RAP\n\n\n\n\nUseful materials\n\n\n\n\n Back to top",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 1: Introduction to RAP"
    ]
  }
]