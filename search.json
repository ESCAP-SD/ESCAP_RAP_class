[
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "Warning\n\n\n\nThis page is still under construction and some sections may not be fully complete\n\n\n\n\nAs part of the Big Data Project, over the summer of 2024, ESCAP has been providing capacity support to project countries that have indicated web scraping for price statistics as a priority – by providing training on web scraping. Check out the rich training website for more details about the training in general, watch videos of past sessions, and review the materials.\nA key topic of this training is Reproducible Analytical Pipelines or RAP. RAP was originally developed in the United Kingdom government to improve the processes that government teams use to make their outputs more reproducible, as well as speed up their production processes. RAP is really a framework – that brings together a range of open source tools, techniques from fields like reproducible research, software engineering, and DevOps to make statistical releases easily reproducible, testable, and auditable (check out the original article from 2017explaining the concept and the purpose). Given that its a framework for mature processes, it can also be applied to the critical aspect of web scraping – as data collection and preparation for the CPI should be robust and highly reproducible!\nThe purpose of this course is thus to teach RAP to project countries and help them apply the concept to web scraping, but also help them practice the techniques and tools so that RAP can be applied to other pipelines to produce the CPI or other official statistics.\n\n\n\nThis site is used to structure and store the RAP training materials. Specifically, it aims to:\n\nSummarize how the training is structured and provide copies of the training slides\n\nCheck out the training scope;\nAs well as the slides for the September 11, 2024 session, as well as the September 18, 2024 session\n\nIt contains an example pipeline to scrape a website and create an output data file. This example acts as a demonstration of a production piece of software to demonstrate how jupyter notebooks that were heavily demonstrated in the virtual training are transformed into a more mature process that can be run regularly to support the CPI:\n\nThe site thus demonstrates various RAP principles in action in a simple way. This documentation is thus meant to show the key concept, but not necessarily be comprehensive.\nThe site also stores the code to operate the scraping RAP and documents its operational aspects\n\nThe site also provides a visual overview (via a process map) how a web scraper pipeline such as the one demonstrated can be integrated into other aspects of the CPI production process, commenting on how RAP can be applied to other aspects.\n\n\n\n\nThis site (and the training) does not aim to be comprehensive but simply demonstrate and document the key concepts. For more on RAP, we encourage you to check out:\n\nThe NHS RAP Communitity of Practice – that contains lots more useful content\nThe RAP Companion - which provides a good guide for each component (although focuses on R);\nThe Udemy class on Reproducible Analytical Pipelines (RAP) using R- provides a good overview\nA presentation and a paper providing a good guide of the application of RAP to price statistics, specifically for rail fares\nand many more!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#context",
    "href": "docs/index.html#context",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "As part of the Big Data Project, over the summer of 2024, ESCAP has been providing capacity support to project countries that have indicated web scraping for price statistics as a priority – by providing training on web scraping. Check out the rich training website for more details about the training in general, watch videos of past sessions, and review the materials.\nA key topic of this training is Reproducible Analytical Pipelines or RAP. RAP was originally developed in the United Kingdom government to improve the processes that government teams use to make their outputs more reproducible, as well as speed up their production processes. RAP is really a framework – that brings together a range of open source tools, techniques from fields like reproducible research, software engineering, and DevOps to make statistical releases easily reproducible, testable, and auditable (check out the original article from 2017explaining the concept and the purpose). Given that its a framework for mature processes, it can also be applied to the critical aspect of web scraping – as data collection and preparation for the CPI should be robust and highly reproducible!\nThe purpose of this course is thus to teach RAP to project countries and help them apply the concept to web scraping, but also help them practice the techniques and tools so that RAP can be applied to other pipelines to produce the CPI or other official statistics.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#how-the-site-can-help-you-and-guide-to-navigating-it",
    "href": "docs/index.html#how-the-site-can-help-you-and-guide-to-navigating-it",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "This site is used to structure and store the RAP training materials. Specifically, it aims to:\n\nSummarize how the training is structured and provide copies of the training slides\n\nCheck out the training scope;\nAs well as the slides for the September 11, 2024 session, as well as the September 18, 2024 session\n\nIt contains an example pipeline to scrape a website and create an output data file. This example acts as a demonstration of a production piece of software to demonstrate how jupyter notebooks that were heavily demonstrated in the virtual training are transformed into a more mature process that can be run regularly to support the CPI:\n\nThe site thus demonstrates various RAP principles in action in a simple way. This documentation is thus meant to show the key concept, but not necessarily be comprehensive.\nThe site also stores the code to operate the scraping RAP and documents its operational aspects\n\nThe site also provides a visual overview (via a process map) how a web scraper pipeline such as the one demonstrated can be integrated into other aspects of the CPI production process, commenting on how RAP can be applied to other aspects.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/index.html#useful-reading",
    "href": "docs/index.html#useful-reading",
    "title": "Web Scraping for the CPI: Training on Reproducible Analytical Pipelines (RAP)",
    "section": "",
    "text": "This site (and the training) does not aim to be comprehensive but simply demonstrate and document the key concepts. For more on RAP, we encourage you to check out:\n\nThe NHS RAP Communitity of Practice – that contains lots more useful content\nThe RAP Companion - which provides a good guide for each component (although focuses on R);\nThe Udemy class on Reproducible Analytical Pipelines (RAP) using R- provides a good overview\nA presentation and a paper providing a good guide of the application of RAP to price statistics, specifically for rail fares\nand many more!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_18/sept_18_session.html",
    "href": "docs/teaching_materials/sept_18/sept_18_session.html",
    "title": "Session 2: Deeper dive into key components",
    "section": "",
    "text": "Note\n\n\n\nThis page will be updated ahead of the September 18th session",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 2: Deeper dive into key components"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_18/sept_18_session.html#presentation-for-the-session",
    "href": "docs/teaching_materials/sept_18/sept_18_session.html#presentation-for-the-session",
    "title": "Session 2: Deeper dive into key components",
    "section": "Presentation for the session",
    "text": "Presentation for the session\n\n\n\nPresentation on RAP",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 2: Deeper dive into key components"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_18/sept_18_session.html#practical-set-one",
    "href": "docs/teaching_materials/sept_18/sept_18_session.html#practical-set-one",
    "title": "Session 2: Deeper dive into key components",
    "section": "Practical content, first set",
    "text": "Practical content, first set\n\nPrinciple 1: Automation\nAs the fundamental principles of RAP is to make the process of (in our case) making official statistics reproducible, we should do our utmost to eliminate manual human steps (unless they are explicitly needed off course!). Thus we want to create a script that automates the whole process and creates a very neat handoff of a tidy data file as output. A key aspect here is mapping the process so that your script can operate in a way as to minimize human actions.\nIn our case, we’ve seen this with the overall process of the price statistics pipeline with web scraped data, as well as the specific step that maps the process of web scraping.\nWe’ll see some practical steps below (once we’ve covered modular coding) on how that could work in practice.\n\n\nPrinciple 2: Modular, re-usable\nModularity and coupling are two very critical aspects and we should spend time discussing them in detail. They actually are conceptually quite related and for simplicity (in this course), we can consider them as trying to achieve the same thing. These are also both far from a RAP (or even official statistics concept) as they are widely discussed in the wider software engineering and even software architecture! This should not discourage us though - as knowing these is also important for us for two aspects:\n\nIt helps us write better code when we are developing pipelines, and\nIt also helps us design more dynamic overall processes of how we build operational flows with alternative data.\n\nSo what is the concept?\n\n\n\ntight vs loose coupling\n\n\nIf we write everything all in one long and big script - it is likely that some small thing will break the script and we will have to spend a long time figuring out what is wrong and fixing it. However, if we isolate specific logical steps into functions and then put them together - if something happens, then we will quickly be able to isolate it to the affected function and fix that function. Each function is thus meant to do just one thing - but it does that one thing very well!\nWe should thus aim to develop loosely coupled software.\nFurthermore, we can also apply this thinking to bigger processes as we saw when we mapped the overall process of the price statistics pipeline with web scraped data - we broke out the overall process into several steps - in essence separate pipelines.\n\n\nWhat does this mean if we put it together?\nLets see what this means by focusing on the example provided by the NHS - what could this look like in code?\nRemember the demo from the RAP guide:\n\n\n\n\n\ngraph LR\n\n    A[Load CSV] --&gt; B[(Database)];\n    H[/CSV 1/] --&gt; A\n    I[/CSV 2/] --&gt; A\n    D[Calculate todays date] --&gt; E[[Run SQL Script]]\n    K[Get todays date manual/config]:::manual --&gt; E\n    J{Is todays date provided?} --&gt;|no| D\n    J --&gt;|yes| K\n    B --&gt; E\n    E --&gt; F[Generate Excel output]\n    F --&gt; G[/Excel Output/]\n\n    classDef manual fill:#FFCCCC\n\n\n\n\n\n\nLet’s see what general functions could be created\ndef load_csv():\n    # do something to load the csv and convert it to a clean dataframe\n    return dataframe\n\ndef load_database(dataframe):\n    # load the data in the dataframe into the database\n\ndef output_date_for_processing(optional_date_input):\n    # check if the input data is provided, if not, calculate it\n    return todays_date\n\ndef run_sql_script(todays_date):\n    # Run the SQL script that we need to run\n    return dataframe_output\n\ndef save_excel_output(dataframe, location_to_save_output):\n    # Save the output in the location we want\nNow we can figure out how to automate this. A very simple way when you are starting out is to make one main() function that calls other functions, and then give inputs to that function for it to operate properly:\ndef main(csv_to_load_into_the_database, date, location_to_save_output):\n    # main function that operationalizes everything\n\n    # Step 1:\n    # load the csv\n    dataframe = load_csv(csv_to_load_into_the_database)\n    # save the data into the databse\n    load_database(dataframe)\n\n    # Step 2:\n    # check date\n    date_to_use = output_date_for_processing(date)\n    # send this to SQL script\n    dataframe_from_sql = run_sql_script(date_to_use)\n    # save this output\n    save_excel_output(dataframe_from_sql)\n\n\n# -----------------------------------------------------------------------\ncsv1 = \"C:/some/file/path/file.csv\"\ntodays_date = '2024-09-18'\nlocation_to_save = \"C:/some/other/path\"\n# lets call the main function\n\nmain(csv1, todays_date, location_to_save)\nThat’s it! Check out a helpful article by the NHS on functional programming for a slighly deper dive.\n\n\nExercise 1\nReady to try this yourself? Look at a notebook that you developed for this course - and see how to separate out what you did into functions. Talk to the mentors in the room if you have questions!",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 2: Deeper dive into key components"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_18/sept_18_session.html#practical-set-two",
    "href": "docs/teaching_materials/sept_18/sept_18_session.html#practical-set-two",
    "title": "Session 2: Deeper dive into key components",
    "section": "Practical content, second set",
    "text": "Practical content, second set\n\nPrinciple 3: Transparency\nWorking in a transparent manner (even if in your NSO this will apply to a more internal audience rather than publicly), is very useful as it helps develops skills and a way of working that results in better code and hence easier (and more reproducible) processes. This principle actually links quite heavily with several other principles such as:\n\nuse of a git based version control software to store an authoritative version of the code and all related aspects (this does not have to be GitHub, GitLab for example is also very popular);\nusing open-source tools (principle 4)\nadopting good development practices (which includes good coding practices and also documentation of how the code works) (principle 6)\nand others!\n\nWhile at a glance, this principle seems to be repeated in other principles - however transparency stands as a principle on its own. Adopting all other principles but still working in a non-transparent manner will naturally result in lower trust (as noone will know how the specific process works), lower levels of collaboration (and thus higher likelihood that mistakes or improvements will be identified), and a very strong likelihood of isolation from consistent standards others have adopted. Thus working in a transparent manner (as much as possible in your NSO) is very useful and helps push the pipeline to be even more mature and reproducible!\n\n\nPrinciple 4: Use of open-source tools\nAs RAP fundamentally is about reproducibility and technical maturity - both aspects are most effectively enabled if open-source tools are used. Open-source tools (like python or R) are incredibly popular around the world - hence there is lots of training on the topic, ChatGPT will give you a good response for specific problems, and its more likely that your colleagues can learn it (or have already) and can help you develop the pipeline or peer review it!\nAs an aside - several organizations (for example see IEEE specturm and Statista) rate the top programming languages around the world and Python (R less so but still a bit) usually are rated highly. The challenge with these surveys is they assess or survey all developers - and some developers need to focus on back-end (SQL or Java) or web development (hence JavaScript) - which can seem misleading. A good takeaway is that for dynamic and simple languages for data analysis and processing - Python and R are very popular - which helps when you are developing your pipeline!\n\n\nPrinciple 5: Version control\nRAPs (and code that helps create official statistics) if fundamentally well developed open-sources software. Hence we need to adopt tools that help us manage all the aspects of its lifecycle - and git based version control with a site like GitHub or GitLab is the most popular way to help develop and operates software. As a summary it allows you to:\n\nversion control all your code and documents (and data too for non-confidential data) - thus you never have to know which is the final one or which was used when in the past!\n\nhave a look at the commits to the main branch of this repo!\n\nfurthemore, from a development point of view, you can also create targeted releases for stable versions and see what the release was at that time - making it very easy to know what version of the pipeline was used historically!\n\nhave a look at the releases for the python pandas library, very popular with data analysis package!\n\ncreate different code bases (as braches) off the main one that allow you to collaborate with others or develop/fix parts simultaneously, as well as join them all together when appropriate;\n\nfor example, have a look at a pull request (a request to merge one branch into the main branch) that we collaborated on for this class - where you can have one individual review the change and approve it prior to the change happening!\n\nproject manage all the aspects of the work by creating issues (for each task or bug), milestones (for phases or categories of the work).\n\ncheck out the GitHub project we used to coordinate the development of materials for this class!\n\nautomate testing and deployment of your package or its related aspects automatically (an advanced feature but quite useful).\n\nfor example have a look at how we enabled GitHub to update this class website automatically and deploy a new version every time we make changes to the main branch\n\nand many more!\n\nWe covered git and GitHub/Lab in detail in the slides (above), however for more details, feel free to check out the NHS intro to Git for more details.\n\n\nExercise 2 (on your own)\nReady to try this yourself? Let’s try to create a very simple GitHub repository. To walkthrough this excersize, you need to have git installed on your computer and you need a GitHub account. Do these steps first if you don’t already have this.\nThere are several exercises we recommend you try at your own pace. As these may take some time, this can be done in the afternoon:\n\nGitHub has a very very simple Hello World example - https://docs.github.com/en/get-started/start-your-journey/hello-world. Follow that to create a repository!\nAs the above example did not have a copy of the code locally, a slightly more involved walkthrough is available from the NHS RAP community of practice on committing work to a remote repository.\nThe NHS RAP Community of Practice also has a walkthrough on how to work with branches",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 2: Deeper dive into key components"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_18/sept_18_session.html#practical-set-three",
    "href": "docs/teaching_materials/sept_18/sept_18_session.html#practical-set-three",
    "title": "Session 2: Deeper dive into key components",
    "section": "Practical content, third set",
    "text": "Practical content, third set\nNow we are in the home stretch! We are starting to see how we can make better software (i.e. the RAP way), but there are three more principles to go!\n\nPrinciple 6: Good development practices\nThere are a few good practices that you should adopt when you develop:\n\nWhen you start out, map your process so that the code is well defined and logical. We covered this in the first principle, however its applicable to good development practices as well designed code will be clear! It will thus help any colleagues looking (or reviewing) at the code (soon or later in the future), but it will also help you as it will be much easier to understand what is going on;\nDocument your code and the overall process the RAP is automating! The RAP way (and the way of all good open-source code) is that the documentation is quite often embedded into the repository where the code is and is part of the delivery. Thus if the code or process changes, the documentation is updated. For example, the RAP Companion exemplar package contains all the key components (the code (i.e. functions), tests (we will cover this later), documentation, and the data (if its not confidential)):\n\nThe process map you developed can thus be embedded into this documentation and be useful to anyone!\nFor instance for this class (as its a repository) stores all docs as qmd files (which are a type of quarto markup file that renders into the beautiful html site by quarto). Have a look at the raw documentation here - https://github.com/sergegoussev/ESCAP_RAP_class/tree/main/docs\nUse py files to store your production pipeline and an Integrated Development Environemnt (IDE) such as VS Code, not a jupyter notebook. This follows from principle 2 (modular code) as we can then group the functions by theme and put them in separate .py files - which keeps your code logical and clear. A way this could be considered is converting the whole pipeline into a package (like BeautifulSoup or requests that we used to learn how to scrape!)\n\nCheck out this guide on packaging python code to make it a package.\n\nUse logging instead of print(). When we first start out and especially in a jupyter notebook - we use print() a lot to see what is going on, especially when the logic is complex. However when we create a production pipeline - we still want to know about many of these scenarios (like something that happened). However we should not print() the statement as it is very hard to read and later analyze the example. Instead we can save all that in a light .log file\n\nCheck out the guide on logging by the NHS RAP site\n\nAnticipate and handle errors. Sometimes something unanticipated happens (you probably encountered lots of cases during development in this web scraping course) - which causes your script to crash or error out. This will likely happen when your pipeline runs, but by anticipating this, you can handle the situations gracefully. For instance we may not want the script to crash but just log that something unanticipated happened. Or we may want the process to stop but the error we want to occur should be very clear and explicit about what went wrong!\n\nCheck out this guide on error handling by the NHS RAP site\n\nWe covered functional programming above - but further to this, you can include documentation for what each function does. For example we have a function that scrapes a site. We can very clearly document how the function works, what it expects as an input, what it returns as an output:\ndef scrape_url(\n    url: str, \n    header: dict, \n    session: requests.sessions.Session, \n    logger: logging.Logger) -&gt; BeautifulSoup:\n\"\"\"\nScrapes a specific URL and logs that the site was scraped.\n\nTakes the input temperature (in Fahrenheit), calculates the value of\nthe same temperature in Celsius, and then returns this value.\n\nArgs:\n    url: a string representing the url to scrape\n    header: a header dictionary to include with the request to the site\n    session: the python requests session being used to make the call (for memory optimization)\n    logger: a logging object so that the fact that we called the site can be recorded\n\nReturns:\n    A Beautiful Soup object that you can work with\n\"\"\"\n    with session.get(url, headers=header) as res:\n        response = BeautifulSoup(res.text, \"html.parser\")\n    logger.info(\"Scraped {url}\".format(url=url))\nreturn response\nThis makes it very easy for others (and yourself later too) to use code that is well documented and modular!\nAnd many more best practices that you can learn slowly and at your pace! Have a look at the Python page (NHS source again) to see some good practices features.\n\n\n\nPrinciple 7: Testing\nOnce you’ve converted your code into functions, it is best to create test data and test that the function works exactly as you expect. This is a very useful step as this makes you very certain that all your functions work exactly as expected - meaning that your entire pipeline works as expected. This process should also be automated - which allows you to check the code every time you make a change. If your code doesn’t pass the tests you set out for it - then you can stop short of using it in production and go and fix it!\nTesting is a complex topic - so we won’t cover it in more detail. However we encourage you to check out the unit testing guide on the NHS RAP site.\n\n\nPrinciple 8: Peer review\nPeer review is a very powerful and useful way to ensure that the package works as expected, make improvements, or even work collaboratively! We can do it in several ways, but the Pull request feature is an easy and natural place to do peer review as it allows you to assign reviewers and assignee (say the reviewer needs to approve) the change. It also shows you that peer review is valuable for any change to the pipeline - i.e. it is not something you do once at the end of development. For example:\n\nYou may want to propose something and have your colleague validate the proposed change or modify it as necessary - before it is accepted. For instance when we were developing this training materials, we used the pull request feature to review major proposed chagnes - such as the scope of the course!\nYou may want to have someone review the code to ensure the code is good and it is improving (you never want to have your code deteriorate over time - its instead always good to aim to constantly improve the code). You can also share knowledge as different colleagues with different levels of skills review the code to collaborate and also to share skills. The NHS RAP site, as well as Google’s change log (i.e. every change) guide are great resources to check out on this topic!\n\n\n\nExercise 3\nReady to try this out? Do one of two possible exercises:\n\nExercise 3.1. Peer review exercise\nIf you have a GitHub repo set up and have walked through the above examples (exercise 2), find someone else who has done the same. Steps to follow:\n\nCreate a branch, push changes to that branch, and then create a Pull Request to merge this branch into the main one.\nAssign your collaborator on the repo (you may need to add the person you are working with as a collaborator to the repo - make the change in Settings &gt; Collaborators) to the request.\nYour partner will review and approve the change\nYou can then merge the change into main\n\nResources:\n\nThe guide on Pull and Merge requests by the NHS RAP Community of Practice will walk through how to create and push pull requesets.\n\n\n\nExercise 3.2. Add doc strings to the functions you created in exercise 1",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 2: Deeper dive into key components"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_18/sept_18_session.html#wrap-up",
    "href": "docs/teaching_materials/sept_18/sept_18_session.html#wrap-up",
    "title": "Session 2: Deeper dive into key components",
    "section": "Wrap up",
    "text": "Wrap up\nWhile we’ve covered the main principles of RAP above - it may feel like it will take you a long time to get all this. However, you can start out small and adopt RAP gradually. Indeed, RAP can be seen as a gradual progression along 3 levels:\n\nBronze or baseline RAP - you have adopted some best practices\nSilver - you are now implementing many good practices and your RAP is increasingly mature, reproducible, and effective\nGold - you have reached a level of easy reproducibility and mature code\n\nCheck out the RAP levels on the NHS site for more info of what is in each level.\nBronze is quite a nice starting place, however as you keep progressing, you can gradually adopt features from the silver (or even gold) level - as not all aspects of a specific level need to be adopted all at once!",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 2: Deeper dive into key components"
    ]
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html",
    "href": "docs/applying_rap/process-mapping.html",
    "title": "Applying RAP for price statistics",
    "section": "",
    "text": "As RAP is a set of technical best practices and styles of working, we should first consider how it applies to price statistics.\n\n\nWeb scraping is the first of several phases necessary to create elementary aggregates –- which are the main input and the building blocks of the Consumer Prices Index. The below sections first provide an overview of the key step and options of how RAP cap apply to the overall process, then go through each step in order to gather detailed requirements for the web scraping step.\n\n\nThe below diagram provides an overview of 4 main steps that include the web scraping step, data processing or preparation step, the classification step, and the price index or aggregation step. Each step outputs a dataset that is used as an input for the next step.\n\n\n\n\n\n\nFigure 1: High level overview of the steps to process web scrape data for the CPI\n\n\n\nEach step is made up of one or more sub-components:\n\nThe web scraping step contains the scraping aspect itself,1 but also includes dataset validation that will help make sure that the scraper is operating as it is expected, as well as reports for review;2\nThe data processing step contains the process to standardizing and preparation step.3\nThe classification step contains several sub-steps, such as identifying unique products to classify, the classification method itself, and the manual validation of classification (in cases there are errors).4\nFor the price index step, many sub-steps are involved.5\n\n\n\n\nGiven this context, RAP can be applied in several ways. As RAP is a way to encapsulate the creation of a statistical (in our case) process into one corpus (i.e. repository with all contexts) – we can:\n\nEncapsulate the whole process in one repository. As RAP is meant to help minimize coupling (dependencies) between separate processes – treating the whole process end-to-end as one RAP is useful if no steps in this pipeline have to be shared with other pipelines.\nEncapsulate each step into a separate RAP. This may be appropriate if several processing phases are done in sequence with specific targeted stopping points – such as handoff between teams or if some steps are shared or where manual steps are necessary. The UK RAP implementation shows how several pipelines operate on top of a data architecture in sequence.6\nEncapsulate as appropriate. Several steps could be combined to simplify the process and if separation is not required, such as the web scraping and data processing steps.\n\nFor this guide and for the demo RAP scraper, we will follow approach (b) as this will allow us to develop RAP for just the scraping component.\n\n\n\n\nLet’s look a little closer at the web scraping component as there is a little more to it than one scrape script that ouptuts the data and a report. We can portray this visually to flush out a bit more what is involved in this step:\n\n\n\nAs we discussed in the course and in other sources, the legal aspects of scraping is key as NSOs should follow the guidance of retailers whether they can be scraped or not. This involves checking the Terms and Conditions for the site, but also checking the robots.txt for the site. As the retailer site may change its robots.txt, it maybe useful to have an automated component that checks before starting the scrape so that any new exclusions are taken into account.\n\n\n\nThe scraper itself is usually custom to the retailer site and outputs a file for what was scraped that day. Using RAP principles to develop this well (which we cover in the September 18th session and provide examples of how a notebook can be translated into something more RAP friendly) are off course key so that its stable and when inevitable website changes happen - the scraper can be fixed quickly.\n\n\n\nTechnically a component of the scraping pipeline itself, however a very valuable part of the scraper worth pointing out separately is logging the operation of the scrape. There may be legal requirements (such as the robots.txt asking for a specific delay in scraping the site, or your NSO requiring specific processes), as well as technical (such as easier debugging) reasons where having key things logged in a .log file useful to describe the operations of the scraper. It can also be combined with the monitoring component to better understand when something goes wrong.\n\n\n\nAs a retailer can choose to change their website at any time - monitoring that the intended data was collected is key. Specifically, logs or error handling in the scraper component may not capture all changes to the site - such as because the retailer chose to stop putting some products on their website. These changes can thus fail the previous checks silently, but the resultant scrape may now no longer be of the same coverage that the NSO expects - which would impact the statistics calculated with this data. Thus NSOs tend to stand up monitoring for an extra layer of validation. This check is also similar in nature to the checks that NSOs tend to do on scanner data that is received regularly.7",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html#overview-of-the-ads-processing-view",
    "href": "docs/applying_rap/process-mapping.html#overview-of-the-ads-processing-view",
    "title": "Applying RAP for price statistics",
    "section": "",
    "text": "Web scraping is the first of several phases necessary to create elementary aggregates –- which are the main input and the building blocks of the Consumer Prices Index. The below sections first provide an overview of the key step and options of how RAP cap apply to the overall process, then go through each step in order to gather detailed requirements for the web scraping step.\n\n\nThe below diagram provides an overview of 4 main steps that include the web scraping step, data processing or preparation step, the classification step, and the price index or aggregation step. Each step outputs a dataset that is used as an input for the next step.\n\n\n\n\n\n\nFigure 1: High level overview of the steps to process web scrape data for the CPI\n\n\n\nEach step is made up of one or more sub-components:\n\nThe web scraping step contains the scraping aspect itself,1 but also includes dataset validation that will help make sure that the scraper is operating as it is expected, as well as reports for review;2\nThe data processing step contains the process to standardizing and preparation step.3\nThe classification step contains several sub-steps, such as identifying unique products to classify, the classification method itself, and the manual validation of classification (in cases there are errors).4\nFor the price index step, many sub-steps are involved.5\n\n\n\n\nGiven this context, RAP can be applied in several ways. As RAP is a way to encapsulate the creation of a statistical (in our case) process into one corpus (i.e. repository with all contexts) – we can:\n\nEncapsulate the whole process in one repository. As RAP is meant to help minimize coupling (dependencies) between separate processes – treating the whole process end-to-end as one RAP is useful if no steps in this pipeline have to be shared with other pipelines.\nEncapsulate each step into a separate RAP. This may be appropriate if several processing phases are done in sequence with specific targeted stopping points – such as handoff between teams or if some steps are shared or where manual steps are necessary. The UK RAP implementation shows how several pipelines operate on top of a data architecture in sequence.6\nEncapsulate as appropriate. Several steps could be combined to simplify the process and if separation is not required, such as the web scraping and data processing steps.\n\nFor this guide and for the demo RAP scraper, we will follow approach (b) as this will allow us to develop RAP for just the scraping component.",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html#closer-look-at-the-web-scraping-component",
    "href": "docs/applying_rap/process-mapping.html#closer-look-at-the-web-scraping-component",
    "title": "Applying RAP for price statistics",
    "section": "",
    "text": "Let’s look a little closer at the web scraping component as there is a little more to it than one scrape script that ouptuts the data and a report. We can portray this visually to flush out a bit more what is involved in this step:\n\n\n\nAs we discussed in the course and in other sources, the legal aspects of scraping is key as NSOs should follow the guidance of retailers whether they can be scraped or not. This involves checking the Terms and Conditions for the site, but also checking the robots.txt for the site. As the retailer site may change its robots.txt, it maybe useful to have an automated component that checks before starting the scrape so that any new exclusions are taken into account.\n\n\n\nThe scraper itself is usually custom to the retailer site and outputs a file for what was scraped that day. Using RAP principles to develop this well (which we cover in the September 18th session and provide examples of how a notebook can be translated into something more RAP friendly) are off course key so that its stable and when inevitable website changes happen - the scraper can be fixed quickly.\n\n\n\nTechnically a component of the scraping pipeline itself, however a very valuable part of the scraper worth pointing out separately is logging the operation of the scrape. There may be legal requirements (such as the robots.txt asking for a specific delay in scraping the site, or your NSO requiring specific processes), as well as technical (such as easier debugging) reasons where having key things logged in a .log file useful to describe the operations of the scraper. It can also be combined with the monitoring component to better understand when something goes wrong.\n\n\n\nAs a retailer can choose to change their website at any time - monitoring that the intended data was collected is key. Specifically, logs or error handling in the scraper component may not capture all changes to the site - such as because the retailer chose to stop putting some products on their website. These changes can thus fail the previous checks silently, but the resultant scrape may now no longer be of the same coverage that the NSO expects - which would impact the statistics calculated with this data. Thus NSOs tend to stand up monitoring for an extra layer of validation. This check is also similar in nature to the checks that NSOs tend to do on scanner data that is received regularly.7",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/applying_rap/process-mapping.html#footnotes",
    "href": "docs/applying_rap/process-mapping.html#footnotes",
    "title": "Applying RAP for price statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nndefinedSee Practical guidelines on web scraping for the HICP (2020), specifically Annex I and VII for mor↩︎\nSee Monitoring, validation and plausibility checks for web scraped data (UN e-Handbook) for more details↩︎\nSee Preparation of data (UN e-Handbook) for more handbook.↩︎\nFor an overview of Classification process in production and other operational aspects, see “Classification of Alternative Data Sources”, 2024-05-14. For an overview of the 5 main classification methods, see “Classifying Alterantive Data Sources for Consumer Prices Statistics: Methods and best practices”, 2023-06-08.↩︎\nSee “Practical guidelines on web scraping for the HICP (2020)” for more details↩︎\nSee Price (2023) Developing reproducible analytical pipelines for the transformation of consumer price statistics: rail fares for more details, for instance 5.2 outlines Pipeline tables and how different pipelines interact over a specific data architecture.↩︎\nGuðmundsdóttir and Jónasdóttir (2016) Scanner Data: Initial Testing provide a useful overview that could be similar in the web scraping case.↩︎",
    "crumbs": [
      "Applying RAP for price statistics"
    ]
  },
  {
    "objectID": "docs/404.html",
    "href": "docs/404.html",
    "title": "Page Not Found",
    "section": "",
    "text": "The page you requested cannot be found (perhaps it was moved or renamed).\nGo back to the home page or search for what you are looking for\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/scraper_docs/output_data.html",
    "href": "docs/scraper_docs/output_data.html",
    "title": "ESCAP training on RAP",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Output Data"
    ]
  },
  {
    "objectID": "docs/scraper_docs/modularity.html",
    "href": "docs/scraper_docs/modularity.html",
    "title": "Overview of modules",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Overview of modules"
    ]
  },
  {
    "objectID": "notebooks/scraping_experiment.html",
    "href": "notebooks/scraping_experiment.html",
    "title": "Web scraping example",
    "section": "",
    "text": "Similar to other examples that we had in the course - this notebook will demo how to scrape Books to scrape - a fictional demo site that shows book prices and is designed for teaching web scraping! As it looks like a retailer website!\n\n\nBefore staring out - we normally check whether we can scrape the site. To validate whether this site allows scraping - let’s check this as well. This way we also demonstrate common steps: * Firstly we can check https://books.toscrape.com/robots.txt for the site - which does not exist! * Secondly we check the Terms and Conditions on the site - however this site does not have anything. Searching the internet tells us that this is a popular scraping sandbox for beginners!\nThus we can proceed!",
    "crumbs": [
      "Applying RAP for price statistics",
      "Web scraping example"
    ]
  },
  {
    "objectID": "notebooks/scraping_experiment.html#can-we-scrape-at-all",
    "href": "notebooks/scraping_experiment.html#can-we-scrape-at-all",
    "title": "Web scraping example",
    "section": "",
    "text": "Before staring out - we normally check whether we can scrape the site. To validate whether this site allows scraping - let’s check this as well. This way we also demonstrate common steps: * Firstly we can check https://books.toscrape.com/robots.txt for the site - which does not exist! * Secondly we check the Terms and Conditions on the site - however this site does not have anything. Searching the internet tells us that this is a popular scraping sandbox for beginners!\nThus we can proceed!",
    "crumbs": [
      "Applying RAP for price statistics",
      "Web scraping example"
    ]
  },
  {
    "objectID": "notebooks/scraping_experiment.html#navigation-and-how-to-get-the-data",
    "href": "notebooks/scraping_experiment.html#navigation-and-how-to-get-the-data",
    "title": "Web scraping example",
    "section": "Navigation and how to get the data",
    "text": "Navigation and how to get the data\nFirstly, using the developer mode in the browser, we see that there doesn’t seem to be an API behind this site as only the html is delivered. Thus we will need to use BeautifulSoup and scrape the full html.\nTo navigate the html aspect of the site, it looks like there are two ways we can do it:\n\nSingle-shot approach and pagination\nIt looks like off the bat, the site lists all books together with a next page at the bottom right.\nWhen we start on page 1, within the html we see a pager class and next class within it:\n\n \nOnce we get to the end, we see only a previous and the next class is no longer there\n\n\n\nThe URL also starts behaves in a stable way, whether for all books or for a specific categories: * https://books.toscrape.com/catalogue/category/books_1/page-2.html * https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html\nThis means that we can iterate through the pages quite easily. Since all the books are also available from the main page - we could just iterate through all 50 pages and get all 1000 books on the site.\n\n\nCategory-by-category approach\nWe can also navigate the category section on the left and then iterate through all the pages for the category:\n\n\nThe html class for this is side_categories and we should easily be able to get into the unordered list and iterate through all of the categories:\n \n\n\nWhich way should we go?\nAs many retailer websites around the world NSOs will need to scrape will likely not get a single home page that lists all products - the more realistic scenario is to iterate first through each category, and then through each page on that category.",
    "crumbs": [
      "Applying RAP for price statistics",
      "Web scraping example"
    ]
  },
  {
    "objectID": "notebooks/scraping_experiment.html#scraping-info-on-each-individual-book-page",
    "href": "notebooks/scraping_experiment.html#scraping-info-on-each-individual-book-page",
    "title": "Web scraping example",
    "section": "Scraping info on each individual book page",
    "text": "Scraping info on each individual book page\nOn each individual page, there are likely several categories that are of interest to us for consumer price statistics: * Product name * Product category * Product description * UPC * Final (post-tax) price * Whether the product is available or not\n\nBelow we will go through how to get all this information.",
    "crumbs": [
      "Applying RAP for price statistics",
      "Web scraping example"
    ]
  },
  {
    "objectID": "notebooks/scraping_experiment.html#scraping-categories",
    "href": "notebooks/scraping_experiment.html#scraping-categories",
    "title": "Web scraping example",
    "section": "Scraping categories",
    "text": "Scraping categories\n\n# Specify the URL\nshop_url = \"https://books.toscrape.com/\"\n\n# Use the with clause we learned about (could also be done directly) to collect and parse the site\nwith s.get(shop_url, headers=heads) as res:\n    response = BeautifulSoup(res.text, \"html.parser\")\n\nGiven the navigation we found previously (i.e. the side_categories div class), lets find all the categories:\n\n# Focus just on the section we want\nside_category_section = response.find(\"div\", class_ = \"side_categories\")\n\n# Isolate all the categories using the link tag as they will have a link\ncategories = side_category_section.find_all('a')\n\n# Iterate through all the categories and save the link in a dictionary key-value pair assigned\n# to the name of the category itself\ndictionary_of_categories = {}\nfor category in categories:\n    dictionary_of_categories[category.text.strip()] = category.get('href')\n\n# Now lets see what we were able to scrape\ndictionary_of_categories\n\n{'Books': 'catalogue/category/books_1/index.html',\n 'Travel': 'catalogue/category/books/travel_2/index.html',\n 'Mystery': 'catalogue/category/books/mystery_3/index.html',\n 'Historical Fiction': 'catalogue/category/books/historical-fiction_4/index.html',\n 'Sequential Art': 'catalogue/category/books/sequential-art_5/index.html',\n 'Classics': 'catalogue/category/books/classics_6/index.html',\n 'Philosophy': 'catalogue/category/books/philosophy_7/index.html',\n 'Romance': 'catalogue/category/books/romance_8/index.html',\n 'Womens Fiction': 'catalogue/category/books/womens-fiction_9/index.html',\n 'Fiction': 'catalogue/category/books/fiction_10/index.html',\n 'Childrens': 'catalogue/category/books/childrens_11/index.html',\n 'Religion': 'catalogue/category/books/religion_12/index.html',\n 'Nonfiction': 'catalogue/category/books/nonfiction_13/index.html',\n 'Music': 'catalogue/category/books/music_14/index.html',\n 'Default': 'catalogue/category/books/default_15/index.html',\n 'Science Fiction': 'catalogue/category/books/science-fiction_16/index.html',\n 'Sports and Games': 'catalogue/category/books/sports-and-games_17/index.html',\n 'Add a comment': 'catalogue/category/books/add-a-comment_18/index.html',\n 'Fantasy': 'catalogue/category/books/fantasy_19/index.html',\n 'New Adult': 'catalogue/category/books/new-adult_20/index.html',\n 'Young Adult': 'catalogue/category/books/young-adult_21/index.html',\n 'Science': 'catalogue/category/books/science_22/index.html',\n 'Poetry': 'catalogue/category/books/poetry_23/index.html',\n 'Paranormal': 'catalogue/category/books/paranormal_24/index.html',\n 'Art': 'catalogue/category/books/art_25/index.html',\n 'Psychology': 'catalogue/category/books/psychology_26/index.html',\n 'Autobiography': 'catalogue/category/books/autobiography_27/index.html',\n 'Parenting': 'catalogue/category/books/parenting_28/index.html',\n 'Adult Fiction': 'catalogue/category/books/adult-fiction_29/index.html',\n 'Humor': 'catalogue/category/books/humor_30/index.html',\n 'Horror': 'catalogue/category/books/horror_31/index.html',\n 'History': 'catalogue/category/books/history_32/index.html',\n 'Food and Drink': 'catalogue/category/books/food-and-drink_33/index.html',\n 'Christian Fiction': 'catalogue/category/books/christian-fiction_34/index.html',\n 'Business': 'catalogue/category/books/business_35/index.html',\n 'Biography': 'catalogue/category/books/biography_36/index.html',\n 'Thriller': 'catalogue/category/books/thriller_37/index.html',\n 'Contemporary': 'catalogue/category/books/contemporary_38/index.html',\n 'Spirituality': 'catalogue/category/books/spirituality_39/index.html',\n 'Academic': 'catalogue/category/books/academic_40/index.html',\n 'Self Help': 'catalogue/category/books/self-help_41/index.html',\n 'Historical': 'catalogue/category/books/historical_42/index.html',\n 'Christian': 'catalogue/category/books/christian_43/index.html',\n 'Suspense': 'catalogue/category/books/suspense_44/index.html',\n 'Short Stories': 'catalogue/category/books/short-stories_45/index.html',\n 'Novels': 'catalogue/category/books/novels_46/index.html',\n 'Health': 'catalogue/category/books/health_47/index.html',\n 'Politics': 'catalogue/category/books/politics_48/index.html',\n 'Cultural': 'catalogue/category/books/cultural_49/index.html',\n 'Erotica': 'catalogue/category/books/erotica_50/index.html',\n 'Crime': 'catalogue/category/books/crime_51/index.html'}\n\n\nFantastic! Appending each category to the site URL will give us a way how to navigate to the category!\nWe can also probably remove the index.html from the end of each as it is detrimental to the perfomance of the site.",
    "crumbs": [
      "Applying RAP for price statistics",
      "Web scraping example"
    ]
  },
  {
    "objectID": "notebooks/scraping_experiment.html#navigate-all-pages-and-save-the-product-book",
    "href": "notebooks/scraping_experiment.html#navigate-all-pages-and-save-the-product-book",
    "title": "Web scraping example",
    "section": "Navigate all pages and save the product (book)",
    "text": "Navigate all pages and save the product (book)\nLets say we want to focus on https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html. as there are 75 books to scrape within this category (others should look the same).\n\nFinding out the product URL so that we can go there later\n\n# Scrape the main site URL + the category we want in the catalogue\ncategory_url = \"catalogue/category/books/mystery_3/index.html\"\nwith s.get(shop_url + category_url, headers=heads) as res:\n    response = BeautifulSoup(res.text, \"html.parser\")\n\n\nproducts = response.find_all(\"article\", class_=\"product_pod\")\n# Extract the first link (which happens to be the picture), although we can get the second link too\nproducts[0].find('a').get('href')\n\n# as the full site per book is \n# https://books.toscrape.com/catalogue/scott-pilgrims-......html\n# we should thus strip ../../.. but keep the catalogue\nshop_url+'catalogue'+products[0].find('a').get('href')[8:]\n\n'https://books.toscrape.com/catalogue/sharp-objects_997/index.html'\n\n\nPerfect! So we now know all the URLs per product\n\n\nFinding the number of pages to go through\nNow we need to know how to iterate through pages\n\n# there is no 'number of pages' to check, but we can estimate the number from \n# the amount of results at the top left of the page. For instance:\nresponse.find(\"form\", class_=\"form-horizontal\").text.strip()\n\n'32 results - showing 1 to 20.'\n\n\n\n# splitting with spaces and then extracting the first item in the list gets us the number by category\nnumber_of_products = response.find(\"form\", class_=\"form-horizontal\").text.strip().split(\" \")[0]\n#knowing that each page displays 20 products and rounding up gives us the number of pages\nround(int(number_of_products)/20)\n\n2\n\n\n\n\nPutting this all together to scrape the whole category\n\n# Scrape the main site URL + the category we want in the catagory of interest\ncategory_url = \"catalogue/category/books/historical-fiction_4/\"\nwith s.get(shop_url + category_url, headers=heads) as res:\n    response = BeautifulSoup(res.text, \"html.parser\")\n\n# create empty list to save URLs of each product/book to scrape into a list\nproducts_to_scrape = []\n\n# Find the number of pages to iterate through:\nnumber_of_products = response.find(\"form\", class_=\"form-horizontal\").text.strip().split(\" \")[0]\n# iterate through the list of pages\nfor page in range(0,math.ceil(int(number_of_products)/20)):\n    # as we start off scraping the category site anyway, then we don't need to scrape it again\n    # however if we are now on page 2, we haven't yet scraped it so we should\n    if page &gt; 0:\n        category_url = category_url + \"page-{}.html\".format(page+1)\n        print('getting',shop_url+category_url)\n        with s.get(shop_url + category_url, headers=heads) as res:\n            response = BeautifulSoup(res.text, \"html.parser\")\n\n    # Find all the product pods on this page\n    products = response.find_all(\"article\", class_=\"product_pod\")\n    # Extract the first link (which happens to be the picture), although we can get the second link too\n    for product in products:\n        href = product.find('a').get('href')\n        products_to_scrape.append(shop_url+'catalogue'+product.find('a').get('href')[8:])\n\ngetting https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html\n\n\n\n# Check the first 3 products\nproducts_to_scrape[3]\n\n'https://books.toscrape.com/catalogue/the-house-by-the-lake_846/index.html'\n\n\n\n# Double check the number sraped\nlen(products_to_scrape)\n\n26\n\n\nPerfect! As this page has 75 products/books - we scraped them all!",
    "crumbs": [
      "Applying RAP for price statistics",
      "Web scraping example"
    ]
  },
  {
    "objectID": "notebooks/scraping_experiment.html#scaping-the-individual-product-page",
    "href": "notebooks/scraping_experiment.html#scaping-the-individual-product-page",
    "title": "Web scraping example",
    "section": "Scaping the individual product page",
    "text": "Scaping the individual product page\n\n# Scrape the main site URL + the category we want in the catagory of interest\ncategory_url = \"catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\"\nwith s.get(shop_url + category_url, headers=heads) as res:\n    response = BeautifulSoup(res.text, \"html.parser\")\n\n\nGet product (i.e. book) title\n\n# get product/book name\nresponse.title.text.split(\"|\")[0].strip()\n\n'Mesaerion: The Best Science Fiction Stories 1800-1849'\n\n\n\n\nGet Product description\n\n# description is challenging as it has no unique id or class, thus we could\n# find the product description tab (which is the first sub-header class) and\n# use the `find_next()` method to get to the description\nprint(response.find_all(\"div\", class_=\"sub-header\")[0].find_next('p').text)\n\nAndrew Barger, award-winning author and engineer, has extensively researched forgotten journals and magazines of the early 19th century to locate groundbreaking science fiction short stories in the English language. In doing so, he found what is possibly the first science fiction story by a female (and it is not from Mary Shelley). Andrew located the first steampunk short Andrew Barger, award-winning author and engineer, has extensively researched forgotten journals and magazines of the early 19th century to locate groundbreaking science fiction short stories in the English language. In doing so, he found what is possibly the first science fiction story by a female (and it is not from Mary Shelley). Andrew located the first steampunk short story, which has not been republished since 1844. There is the first voyage to the moon in a balloon, republished for the first time since 1820 that further tells of a darkness machine and a lunarian named Zuloc. Other sci-stories include the first robotic insect and an electricity gun. Once again, Andrew has searched old texts to find the very best science fiction stories from the period when the genre automated to life, some of the stories are published for the first time in nearly 200 years. Read these fantastic stories today!OUR OWN COUNTRY So mechanical has the age become, that men seriously talk of flying machines, to go by steam, --not your air-balloons, but real Daedalian wings, made of wood and joints, nailed to your shoulder, --not wings of feathers and wax like the wings of Icarus, who fell into the Cretan sea, but real, solid, substantial, rock-maple wings with wrought-iron hinges, and huge concavities, to propel us through the air. Knickerbocker Magazine, May 18 ...more\n\n\n\n\nGet product information\nAs this is a table, there are 2 ways of getting this information, via BeautifulSoup and via pandas:\nBeautifulSoup approach\n\n# save the table into a dictionary\nproduct_info = {}\n\n# filter to the product page\nproduct_page = response.find(\"article\", class_=\"product_page\")\n# as this is the only table with rows, we can just get find all rows on the product page\nrows = product_page.find_all('tr')\nfor row in rows:\n    # focus on each row's cells and then save the values\n    cells = row.find_all(['th', 'td'])\n    product_info[cells[0].text] = cells[1].text\n    \n# have a look at what we saved\nproduct_info\n\n{'UPC': 'e30f54cea9b38190',\n 'Product Type': 'Books',\n 'Price (excl. tax)': 'Â£37.59',\n 'Price (incl. tax)': 'Â£37.59',\n 'Tax': 'Â£0.00',\n 'Availability': 'In stock (19 available)',\n 'Number of reviews': '0'}\n\n\npandas approach\n\nall_tables = pd.read_html(shop_url + category_url)\nall_tables[0]\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nUPC\ne30f54cea9b38190\n\n\n1\nProduct Type\nBooks\n\n\n2\nPrice (excl. tax)\n£37.59\n\n\n3\nPrice (incl. tax)\n£37.59\n\n\n4\nTax\n£0.00\n\n\n5\nAvailability\nIn stock (19 available)\n\n\n6\nNumber of reviews\n0\n\n\n\n\n\n\n\n\n# we can convert this to a dictionary by first setting the first \n# column as the index and second by telling pandas to convert it to a dictionary\nall_tables[0].set_index(0).to_dict()[1]\n\n{'UPC': 'e30f54cea9b38190',\n 'Product Type': 'Books',\n 'Price (excl. tax)': '£37.59',\n 'Price (incl. tax)': '£37.59',\n 'Tax': '£0.00',\n 'Availability': 'In stock (19 available)',\n 'Number of reviews': '0'}",
    "crumbs": [
      "Applying RAP for price statistics",
      "Web scraping example"
    ]
  },
  {
    "objectID": "docs/scraper_docs/error_handling.html",
    "href": "docs/scraper_docs/error_handling.html",
    "title": "Error handling",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Error handling"
    ]
  },
  {
    "objectID": "docs/scraper_docs/mapping_the_process.html",
    "href": "docs/scraper_docs/mapping_the_process.html",
    "title": "Mapping the process",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Mapping the process"
    ]
  },
  {
    "objectID": "docs/scraper_docs/mapping_the_process.html#the-overall-process",
    "href": "docs/scraper_docs/mapping_the_process.html#the-overall-process",
    "title": "Mapping the process",
    "section": "The overall process",
    "text": "The overall process\nIf we start out modifying the scraper we developed in a notebook when we tried scaping books to scrape and map what we did there in order to RAPify the process!",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Mapping the process"
    ]
  },
  {
    "objectID": "docs/scraper_docs/logging.html",
    "href": "docs/scraper_docs/logging.html",
    "title": "Logging",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Demo RAP scraper documentation",
      "Logging"
    ]
  },
  {
    "objectID": "docs/scraper_docs.html",
    "href": "docs/scraper_docs.html",
    "title": "Demo RAP scraper documentation",
    "section": "",
    "text": "Note\n\n\n\nThis page is still under construction",
    "crumbs": [
      "Demo RAP scraper documentation"
    ]
  },
  {
    "objectID": "docs/scraper_docs.html#baseline-rap---getting-the-fundamentals-right",
    "href": "docs/scraper_docs.html#baseline-rap---getting-the-fundamentals-right",
    "title": "Demo RAP scraper documentation",
    "section": "Baseline RAP - getting the fundamentals right",
    "text": "Baseline RAP - getting the fundamentals right\nIn order for a publication to be considered a reproducible analytical pipeline, it must at least meet all of the requirements of Baseline RAP:\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\nCode is version controlled (see Git basics and using Git collaboratively guides).\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code (use NHS Open Source Policy section on Readmes as a guide.\nCode has been peer reviewed.\nCode is published in the open and linked to & from accompanying publication (if relevant).",
    "crumbs": [
      "Demo RAP scraper documentation"
    ]
  },
  {
    "objectID": "docs/scraper_docs.html#silver-rap---implementing-best-practice",
    "href": "docs/scraper_docs.html#silver-rap---implementing-best-practice",
    "title": "Demo RAP scraper documentation",
    "section": "Silver RAP - implementing best practice",
    "text": "Silver RAP - implementing best practice\nMeeting all of the above requirements, plus:\n\nOutputs are produced by code with minimal manual intervention.\nCode is well-documented including user guidance, explanation of code structure & methodology and docstrings for functions.\nCode is well-organised following standard directory format.\nReusable functions and/or classes are used where appropriate.\nCode adheres to agreed coding standards (e.g PEP8, style guide for Pyspark).\nPipeline includes a testing framework (unit tests, back tests).\nRepository includes dependency information (e.g. requirements.txt, PipFile, environment.yml).\nLogs are automatically recorded by the pipeline to ensure outputs are as expected.\nData is handled and output in a Tidy data format.",
    "crumbs": [
      "Demo RAP scraper documentation"
    ]
  },
  {
    "objectID": "docs/teaching_materials/teaching.html",
    "href": "docs/teaching_materials/teaching.html",
    "title": "ESCAP sesions on RAP",
    "section": "",
    "text": "Note\n\n\n\nThis pages provides an overview of the course, but the actual material is still being finalized\n\n\nReproducible Analytic Pipelines (or RAP) is taught by UN ESCAP in two sessions. This training includes several components\n\nOverall scope of the training\nRAP is a large topic and it has its own Udemy course, supporting sites (such as the RAP companion), as well as communities of practice. ESCAP can’t thus hope to cover it in full, but instead aims to provide a detailed enough overview so that member countries being trained on web scraping for the CPI can benefit from maturity principles that RAP represents.\nOverall objective\nThe overall objective of the RAP training is to provide an overview of key RAP principles, cover the value it brings to official statistics, as well as train class participants up to figure out just enough so that they can proceed on their own and help bring in RAP it into their own work. Class participants (who ESCAP trains) can thus become trainers in their own National Statistical Organizations (NSOs) - helping their teams and groups continue to learn more about RAP, explore each of the best practices, and learn how to implement it into their research or official statistics (i.e. production). In a more formal sense, this RAP class is thus aims to increase the capacity of participating NSOs to learn RAP and apply it for their continued modernization, and especially the implementation of alternative (web scrape most notably) data for consumer price statistics. Two sessions provide fist an overview and second a deeper dive on RAP\n\n\nSession 1, virtual: Overview of RAP\nDate: September 11, 2024\nObjective: Introduce ideas inherent in RAP and have participants walk away with an understanding why technical maturity skills are important and how RAP can help them mature their organization\nTopics to cover:\n\nWhy RAP is important to learn\nWhat is RAP in more detail and what problem is it aiming to solve\nOutline of key RAP principles and how they would help\nDiscussion on maturity levels - to show that its not all or nothing\nVery brief demo of a use case to demonstrate something and walk away from the more theoretical ideas\nSome caveats to keep in mind and wrap up for what you will see in person!\n\nContent: See the session page for more details.\n\n\nSession 2, in person: Deeper dive into RAP components\nDate: September 18, 2024\nObjective: Building on the initial training on RAP, go deeper into the components to really get the principles across. To help anchor the components, a use case can be used to understand how a little bit more about each aspect, and some hands-on exercises will help learners get a better appreciation of the tasks.\nTopics to cover:\n\nQuick overview to recap the previous session\nDeeper coverage of principles - weaving the use case into each component:\n\nAutomation\nModularity, re-usability, etc\nTransparency\nOpen source tools\nVersion control\nGood coding practices\nTesting\nPeer review\n\nLevels of RAP\nExercises\n\nExercise in detail: To help anchor several of the technical skills in RAP, participants should spend time in person (helped by the mentors who are there in person) try the excercizes that are provided.\nContent: See the session page for more details.\n\n\n\n\n Back to top",
    "crumbs": [
      "ESCAP sesions on RAP"
    ]
  },
  {
    "objectID": "docs/teaching_materials/sept_11/sept_11_session.html",
    "href": "docs/teaching_materials/sept_11/sept_11_session.html",
    "title": "Session 1: Introduction to RAP",
    "section": "",
    "text": "Overview\nThe purpose of this class is to cover what is RAP and why RAP is important.\n\n\nPresentation for the session\n\n\n\nPresentation on RAP\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ESCAP sesions on RAP",
      "Session 1: Introduction to RAP"
    ]
  }
]